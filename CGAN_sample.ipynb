{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "DATASET_CSV = './tinyCelebA/image_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './tinyCelebA/'\n",
    "\n",
    "# 取り扱う属性ラベル\n",
    "TARGET_ATTRIBUTES = ['Blond_Hair', 'Brown_Hair', 'Black_Hair', 'Gray_Hair', 'Eyeglasses', 'Male', 'Young']\n",
    "\n",
    "# 画像サイズ\n",
    "H = 128 # 縦幅\n",
    "W = 128 # 横幅\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 特徴ベクトルの次元数\n",
    "N = 128\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './CGAN_models/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE_G = os.path.join(MODEL_DIR, './face_generator_model.pth') # ジェネレータ\n",
    "MODEL_FILE_D = os.path.join(MODEL_DIR, './face_discriminator_model.pth') # ディスクリミネータ\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイル\n",
    "CHECKPOINT_EPOCH = os.path.join(MODEL_DIR, 'checkpoint_epoch.pkl')\n",
    "CHECKPOINT_GEN_MODEL = os.path.join(MODEL_DIR, 'checkpoint_gen_model.pth')\n",
    "CHECKPOINT_DIS_MODEL = os.path.join(MODEL_DIR, 'checkpoint_dis_model.pth')\n",
    "CHECKPOINT_GEN_OPT = os.path.join(MODEL_DIR, 'checkpoint_gen_opt.pth')\n",
    "CHECKPOINT_DIS_OPT = os.path.join(MODEL_DIR, 'checkpoint_dis_opt.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mylib.basic_layers import Reshape, MinibatchDiscrimination, DiscriminatorAugmentation\n",
    "\n",
    "\n",
    "# Pre-act Residual Block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, sn=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        shortcut_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        main_conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        main_conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        if sn:\n",
    "            # spectral normalization を用いる場合（主にディスクリミネータ用）\n",
    "            self.shortcut = nn.utils.spectral_norm(shortcut_conv) \n",
    "            self.block1 = nn.Sequential(nn.ReLU(), nn.utils.spectral_norm(main_conv1))\n",
    "            self.block2 = nn.Sequential(nn.ReLU(), nn.utils.spectral_norm(main_conv2))\n",
    "        else:\n",
    "            # バッチ正規化を用いる場合（主にジェネレータ用）\n",
    "            self.shortcut = shortcut_conv\n",
    "            self.block1 = nn.Sequential(nn.BatchNorm2d(num_features=in_channels), nn.ReLU(), main_conv1)\n",
    "            self.block2 = nn.Sequential(nn.BatchNorm2d(num_features=out_channels), nn.ReLU(), main_conv2)\n",
    "    def forward(self, x):\n",
    "        s = self.shortcut(x)\n",
    "        h = self.block1(x)\n",
    "        h = self.block2(h)\n",
    "        return h + s\n",
    "\n",
    "\n",
    "# GANジェネレータ用のアップサンプリング層\n",
    "class myUpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myUpsamplingBlock, self).__init__()\n",
    "        self.up = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.rb = ResBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, sn=False)\n",
    "    def forward(self, x):\n",
    "        h = self.up(x)\n",
    "        return self.rb(h)\n",
    "\n",
    "\n",
    "# GANディスクリミネータ用のダウンサンプリング層\n",
    "class myDownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myDownsamplingBlock, self).__init__()\n",
    "        self.rb = ResBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, sn=True)\n",
    "        self.down = nn.AvgPool2d(kernel_size=2)\n",
    "    def forward(self, x):\n",
    "        h = self.rb(x)\n",
    "        return self.down(h)\n",
    "\n",
    "\n",
    "# 顔画像生成ニューラルネットワーク\n",
    "# CGAN生成器（ジェネレータ）のサンプル\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # C: 出力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 出力顔画像の縦幅（32の倍数と仮定）\n",
    "    # W: 出力顔画像の横幅（32の倍数と仮定）\n",
    "    # N: 入力の特徴ベクトル（乱数ベクトル）の次元数\n",
    "    # K: 属性ラベルの種類数\n",
    "    def __init__(self, C, H, W, N, K):\n",
    "        super(Generator, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "\n",
    "        # 属性ラベル情報を処理する全結合層\n",
    "        self.embed = nn.Linear(in_features=K, out_features=N) # 属性ラベル情報を特徴ベクトルと同じ N 次元に拡張\n",
    "\n",
    "        # 属性ラベル情報と特徴ベクトルを連結した後のベクトルをチャンネル数 512, 縦幅 H/32, 横幅 W/32 の特徴マップに変換する層\n",
    "        self.conv0 = nn.Sequential(\n",
    "            Reshape(size=(2*N, 1, 1)),\n",
    "            nn.ConvTranspose2d(in_channels=2*N, out_channels=512, kernel_size=(H//32, W//32), stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # アップサンプリング層1～5\n",
    "        # これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 2 倍になる\n",
    "        # 5つ通すことになるので，最終的には都合 32 倍になる -> ゆえに縦幅 H/32, 横幅 W/32 の特徴マップからスタートする\n",
    "        self.up1 = myUpsamplingBlock(in_channels=512, out_channels=256)\n",
    "        self.up2 = myUpsamplingBlock(in_channels=256, out_channels=128)\n",
    "        self.up3 = myUpsamplingBlock(in_channels=128, out_channels=64)\n",
    "        self.up4 = myUpsamplingBlock(in_channels=64, out_channels=32)\n",
    "        self.up5 = myUpsamplingBlock(in_channels=32, out_channels=32)\n",
    "\n",
    "        # 出力画像生成用の畳込み層\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=C, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        y = self.embed(y) # 属性ラベル情報を N 次元に\n",
    "        h = torch.cat((z, y), dim=1) # 特徴ベクトルと属性ラベル情報を連結 -> トータル256次元に\n",
    "        h = self.conv0(h) # 256次元の特徴ベクトルをチャンネル数 512, 縦幅 H/32, 横幅 W/32 の特徴マップに変換\n",
    "        h = self.up1(h)\n",
    "        h = self.up2(h)\n",
    "        h = self.up3(h)\n",
    "        h = self.up4(h)\n",
    "        h = self.up5(h)\n",
    "        y = torch.tanh(self.conv5(h))\n",
    "        return y\n",
    "\n",
    "\n",
    "# 顔画像が Real か Fake を判定するニューラルネットワーク\n",
    "# CGAN識別器（ディスクリミネータ）のサンプル\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    # C: 入力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 入力顔画像の縦幅（32の倍数と仮定）\n",
    "    # W: 入力顔画像の横幅（32の倍数と仮定）\n",
    "    # K: 属性ラベルの種類数\n",
    "    def __init__(self, C, H, W, K):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 訓練データ量の不足を補うためのデータ拡張（Data Augmentation）処理\n",
    "        self.preprocess = DiscriminatorAugmentation(H, W, p_hflip=0.5, p_vflip=0.4, p_rot=0.4) # 確率0.5で左右反転，確率0.4で上下反転，確率0.4で回転\n",
    "\n",
    "        # ダウンサンプリング層1～5\n",
    "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 1/2 になる\n",
    "        self.down1 = myDownsamplingBlock(in_channels=C+K, out_channels=32)\n",
    "        self.down2 = myDownsamplingBlock(in_channels=32, out_channels=64)\n",
    "        self.down3 = myDownsamplingBlock(in_channels=64, out_channels=128)\n",
    "        self.down4 = myDownsamplingBlock(in_channels=128, out_channels=256)\n",
    "        self.down5 = myDownsamplingBlock(in_channels=256, out_channels=256)\n",
    "\n",
    "        # 平坦化\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # 全結合層1（spectral normalization を使用）\n",
    "        # ダウンサンプリング層1～5を通すことにより特徴マップの縦幅・横幅は都合 1/32 になっているので，\n",
    "        # 入力側のパーセプトロン数は 256*(H/32)*(W/32) = H*W/4\n",
    "        self.fc1 = nn.utils.spectral_norm(nn.Linear(in_features=H*W//4, out_features=256))\n",
    "\n",
    "        # 全結合層2\n",
    "        self.fc2 = nn.Linear(in_features=384, out_features=1)\n",
    "\n",
    "        # Minibatch Discrimination: モード崩壊を回避するための技法の一つ\n",
    "        self.md = MinibatchDiscrimination(in_features=256, out_features=128)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 本来であれば，ディスクリミネータの出力が 0～1 の範囲となるよう，最終層の活性化関数として sigmoid を適用すべきであるが，\n",
    "        # このサンプルコードでは損失関数側で sigmoid 適用することになるので, ここでは最終層で活性化関数を適用しない\n",
    "        y = y.reshape(*y.size(), 1, 1).repeat(1, 1, x.size()[2], x.size()[3]) # 属性ラベル情報を画像と同じ形に拡張\n",
    "        x = self.preprocess(x)\n",
    "        h = torch.cat((x, y), dim=1) # 画像情報とラベル情報を結合\n",
    "        h = self.down1(h)\n",
    "        h = self.down2(h)\n",
    "        h = self.down3(h)\n",
    "        h = self.down4(h)\n",
    "        h = self.down5(h)\n",
    "        h = self.flat(h)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.md(h) # Minibatch Discrimination\n",
    "        z = self.fc2(h) # 上記の通り，最終層では活性化関数なし\n",
    "        return z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "from mylib.utility import save_datasets, load_datasets_from_file\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "    train_dataset, _ = load_datasets_from_file(MODEL_DIR)\n",
    "    if train_dataset is None:\n",
    "        print('error: there is no checkpoint previously saved.')\n",
    "        exit()\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # CSVファイルを読み込み, 訓練データセットを用意\n",
    "    # 今回は，全てのデータを学習用に回す\n",
    "    train_dataset = CSVBasedDataset(\n",
    "        filename = DATASET_CSV,\n",
    "        items = [\n",
    "            'File Path', # X\n",
    "            TARGET_ATTRIBUTES, # Y\n",
    "        ],\n",
    "        dtypes = [\n",
    "            'image', # Xの型\n",
    "            'float', # Yの型\n",
    "        ],\n",
    "        dirname = DATA_DIR,\n",
    "        img_transform = transforms.CenterCrop((H, W)), # 処理量を少しでも抑えるため，画像中央の H×W ピクセルの部分だけを対象とする\n",
    "    )\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    save_datasets(MODEL_DIR, train_dataset)\n",
    "\n",
    "# 訓練データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.loss_functions import GANLoss\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images, to_sigmoid_image, to_tanh_image, autosaved_model_name\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "# 何エポックに1回の割合で学習経過を表示するか（モデル保存処理もこれと同じ頻度で実行）\n",
    "INTERVAL_FOR_SHOWING_PROGRESS = 10\n",
    "\n",
    "# 通常のGANのときと異なり, 本プログラムではジェネレータを毎回更新することにする\n",
    "N_DIS = 1 # この値を例えば 5 にすれば，ジェネレータは5回に1回の割合でしか更新されなくなる\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "gen_model = Generator(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES)).to(DEVICE)\n",
    "dis_model = Discriminator(C=C, H=H, W=W, K=len(TARGET_ATTRIBUTES)).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "gen_optimizer = optim.Adam(gen_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "dis_optimizer = optim.Adam(dis_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, gen_model, gen_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_GEN_MODEL, CHECKPOINT_GEN_OPT, N_EPOCHS, gen_model, gen_optimizer)\n",
    "    _, _, dis_model, dis_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DIS_MODEL, CHECKPOINT_DIS_OPT, N_EPOCHS, dis_model, dis_optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = GANLoss(label_smoothing=True)\n",
    "\n",
    "# 検証の際に使用する乱数ベクトルおよび属性ラベル情報を用意\n",
    "Z_valid = torch.randn(BATCH_SIZE, N).to(DEVICE) # 検証用乱数ベクトル\n",
    "L_valid = torch.zeros(BATCH_SIZE, len(TARGET_ATTRIBUTES)).to(DEVICE) # 検証用属性ラベル情報\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['G loss', 'D loss'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    gen_model.train()\n",
    "    dis_model.train()\n",
    "    sum_gen_loss = 0\n",
    "    sum_dis_loss = 0\n",
    "    n_iter = 1 # 1エポック内でのループ回数を記録する変数（ジェネレータの更新回数を制御するために使用）\n",
    "    for X, L in tqdm(train_dataloader):\n",
    "        for param in gen_model.parameters():\n",
    "            param.grad = None\n",
    "        for param in dis_model.parameters():\n",
    "            param.grad = None\n",
    "        L = L.to(DEVICE) # 属性ラベル情報\n",
    "        Z = torch.randn(len(X), N).to(DEVICE) # 乱数ベクトルを用意\n",
    "        real = to_tanh_image(X).to(DEVICE) # Real画像を用意（to_tanh_image 関数を用い，画素値の範囲が -1〜1 となるように調整しておく）\n",
    "        fake = gen_model(Z, L) # Fake画像を生成（2行上で用意した Z から生成）\n",
    "        fake_cpy = fake.detach() # Fake画像のコピーを用意しておく\n",
    "        ### ジェネレータの学習 ###\n",
    "        if n_iter % N_DIS == 0:\n",
    "            Y_fake = dis_model(fake, L) # Fake画像を識別\n",
    "            gen_loss = loss_func.G_loss(Y_fake)\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "            sum_gen_loss += float(gen_loss) * len(X)\n",
    "        ### ディスクリミネータの学習 ###\n",
    "        for param in dis_model.parameters():\n",
    "            param.grad = None # ジェネレータの学習時の計算した勾配を一旦リセット\n",
    "        Y_real = dis_model(real, L) # Real画像を識別\n",
    "        Y_fake = dis_model(fake_cpy, L) # Fake画像を識別（コピー変数の方を使用）\n",
    "        dis_loss = loss_func.D_loss(Y_fake, as_real=False) + loss_func.D_loss(Y_real, as_real=True)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        sum_dis_loss += float(dis_loss) * len(X)\n",
    "        n_iter += 1\n",
    "    avg_gen_loss = sum_gen_loss * N_DIS / train_size\n",
    "    avg_dis_loss = sum_dis_loss / train_size\n",
    "    loss_viz.add_value('G loss', avg_gen_loss) # 訓練データに対する損失関数の値を記録\n",
    "    loss_viz.add_value('D loss', avg_dis_loss) # 同上\n",
    "    print('generator train loss = {0:.6f}'.format(avg_gen_loss))\n",
    "    print('discriminator train loss = {0:.6f}'.format(avg_dis_loss))\n",
    "    print('')\n",
    "\n",
    "    # 検証（学習経過の表示，モデル自動保存）\n",
    "    if epoch == 0 or (epoch + 1) % INTERVAL_FOR_SHOWING_PROGRESS == 0:\n",
    "        gen_model.eval()\n",
    "        dis_model.eval()\n",
    "        if epoch == 0:\n",
    "            real = to_sigmoid_image(real) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "            show_images(real.to('cpu').detach(), num=32, num_per_row=8, title='real images', save_fig=False, save_dir=MODEL_DIR)\n",
    "        with torch.inference_mode():\n",
    "            fake = gen_model(Z_valid, L_valid) # 事前に用意しておいた検証用乱数と属性ラベル情報からFake画像を生成\n",
    "            #fake = gen_model(torch.randn(BATCH_SIZE, N).to(DEVICE), torch.zeros(BATCH_SIZE, len(TARGET_ATTRIBUTES)).to(DEVICE)) # エポックごとに異なる乱数を使用する場合はこのようにする\n",
    "        fake = to_sigmoid_image(fake) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "        show_images(fake.to('cpu').detach(), num=32, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "        torch.save(gen_model.state_dict(), autosaved_model_name(MODEL_FILE_G, epoch + 1)) # 学習途中のモデルを保存したい場合はこのようにする\n",
    "\n",
    "    # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_GEN_MODEL, CHECKPOINT_GEN_OPT, epoch+1, gen_model, gen_optimizer)\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DIS_MODEL, CHECKPOINT_DIS_OPT, epoch+1, dis_model, dis_optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "gen_model = gen_model.to('cpu')\n",
    "dis_model = dis_model.to('cpu')\n",
    "torch.save(gen_model.state_dict(), MODEL_FILE_G)\n",
    "#torch.save(dis_model.state_dict(), MODEL_FILE_D) # ディスクリミネータも保存したい場合はこのようにする\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "gen_model = Generator(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES))\n",
    "gen_model.load_state_dict(torch.load(MODEL_FILE_G))\n",
    "#gen_model.load_state_dict(torch.load(autosaved_model_name(MODEL_FILE_G, 90))) # 例えば90エポック目のモデルをロードしたい場合は，このようにする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理（正規分布に従って複数の乱数ベクトルをランダムサンプリングし，それをデコーダに通して画像を生成．属性ラベルは固定値で指定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "gen_model = gen_model.to(DEVICE)\n",
    "gen_model.eval()\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 32\n",
    "\n",
    "# 属性ラベルの指定値\n",
    "# このサンプルコードでは TARGET_ATTRIBUTES = ['Blond_Hair', 'Brown_Hair', 'Black_Hair', 'Gray_Hair', 'Eyeglasses', 'Male', 'Young'] と設定しているので，\n",
    "#   'Blond_Hair' = 0, # ブロンド髪ではない\n",
    "#   'Brown_Hair' = 0, # 茶髪ではない\n",
    "#   'Black_Hair' = 1, # 黒髪である\n",
    "#   'Gray_Hair'  = 0, # 白髪ではない\n",
    "#   'Eyeglasses' = 0, # 眼鏡やサングラスをかけていない\n",
    "#   'Male'       = 0, # 男性でない（== 女性）\n",
    "#   'Young'      = 1, # 若い\n",
    "# という意味になる\n",
    "attributes = [0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "# 標準正規分布 N(0, 1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((n_gen, N)).to(DEVICE)\n",
    "\n",
    "# 属性ラベル情報の作成\n",
    "L = torch.tensor([attributes], dtype=torch.float32).repeat((n_gen, 1)).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルと属性ラベルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    Y = gen_model(Z, L)\n",
    "    Y = to_sigmoid_image(Y)\n",
    "    show_images(Y.to('cpu').detach(), num=n_gen, num_per_row=8, title='CGAN_sample_generated_case1', save_fig=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理（乱数ベクトルを一つだけサンプリングし，それをデコーダに通して画像を生成．属性ラベルは，一つの次元を徐々に変化させる形で指定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "gen_model = gen_model.to(DEVICE)\n",
    "gen_model.eval()\n",
    "\n",
    "# ベースとなる属性ラベル\n",
    "base_attributes = [0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "# 上の属性ラベルのうち何番目の属性値を変化させるか\n",
    "# 以下の例は\n",
    "#   - 0番目の属性（ 'Blond_Hair', ベース値 0 ）を 0 から 1 に徐々に変化\n",
    "#   - 2番目の属性（ 'Black_Hair', ベース値 1 ）を 1 から 0 に徐々に変化\n",
    "# という意味になり，すなわち，ブロンド髪から黒髪への属性変化に相当\n",
    "targets = [0, 2]\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 16 # 上で指定した属性ラベルを 0～1 の間で n_gen 段階に変化させる\n",
    "\n",
    "# 標準正規分布 N(0, 1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((1, N)).repeat((n_gen, 1)).to(DEVICE)\n",
    "\n",
    "# 属性ラベル情報の作成\n",
    "L = []\n",
    "for i in range(n_gen):\n",
    "    attributes = copy.deepcopy(base_attributes)\n",
    "    for t in targets:\n",
    "        # t番目の属性値を 0〜1 の範囲でずらす\n",
    "        if base_attributes[t] == 0:\n",
    "            attributes[t] = i / (n_gen - 1)\n",
    "        else:\n",
    "            attributes[t] = 1 - i / (n_gen - 1)\n",
    "    L.append(attributes)\n",
    "L = torch.tensor(L, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルと属性ラベルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    Y = gen_model(Z, L)\n",
    "    Y = to_sigmoid_image(Y)\n",
    "    show_images(Y.to('cpu').detach(), num=n_gen, num_per_row=8, title='CGAN_sample_generated_case2', save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
