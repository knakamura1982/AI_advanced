{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "# 再開モードの場合も, このエポック数の分だけ追加学習される（N_EPOCHSは最終エポック番号ではない）\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "DATASET_CSV = './tinyCelebA/image_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './tinyCelebA/'\n",
    "\n",
    "# 画像サイズ\n",
    "H = 128 # 縦幅\n",
    "W = 128 # 横幅\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 特徴ベクトルの次元数\n",
    "N = 128\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './GAN_models/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE_G = os.path.join(MODEL_DIR, './face_generator_model.pth') # ジェネレータ\n",
    "MODEL_FILE_D = os.path.join(MODEL_DIR, './face_discriminator_model.pth') # ディスクリミネータ\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイル\n",
    "CHECKPOINT_EPOCH = os.path.join(MODEL_DIR, 'checkpoint_epoch.pkl')\n",
    "CHECKPOINT_GEN_MODEL = os.path.join(MODEL_DIR, 'checkpoint_gen_model.pth')\n",
    "CHECKPOINT_DIS_MODEL = os.path.join(MODEL_DIR, 'checkpoint_dis_model.pth')\n",
    "CHECKPOINT_GEN_OPT = os.path.join(MODEL_DIR, 'checkpoint_gen_opt.pth')\n",
    "CHECKPOINT_DIS_OPT = os.path.join(MODEL_DIR, 'checkpoint_dis_opt.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mylib.basic_layers import Reshape, MinibatchDiscrimination, DiscriminatorAugmentation\n",
    "\n",
    "\n",
    "# Pre-act Residual Block\n",
    "# 通常の Residual Block では畳込みの後に活性化関数をかけるのに対し，その順序を逆にして先に活性化関数をかけるようにしたもの\n",
    "# なお，ここではバッチ正規化の代わりに spectral normalization という正規化手法を選択できるようにしている（sn=Trueを指定すると spectral normalization を使用できる）\n",
    "# spectral normalization は GAN の学習の安定化に有効で，基本的にはディスクリミネータで使用する\n",
    "# 参考サイト: https://qiita.com/SZZZUJg97M/items/371f694f05998439bd45 など\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, sn=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        shortcut_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        main_conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        main_conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        if sn:\n",
    "            # spectral normalization を用いる場合（主にディスクリミネータ用）\n",
    "            self.shortcut = nn.utils.spectral_norm(shortcut_conv) \n",
    "            self.block1 = nn.Sequential(nn.ReLU(), nn.utils.spectral_norm(main_conv1))\n",
    "            self.block2 = nn.Sequential(nn.ReLU(), nn.utils.spectral_norm(main_conv2))\n",
    "        else:\n",
    "            # バッチ正規化を用いる場合（主にジェネレータ用）\n",
    "            self.shortcut = shortcut_conv\n",
    "            self.block1 = nn.Sequential(nn.BatchNorm2d(num_features=in_channels), nn.ReLU(), main_conv1)\n",
    "            self.block2 = nn.Sequential(nn.BatchNorm2d(num_features=out_channels), nn.ReLU(), main_conv2)\n",
    "    def forward(self, x):\n",
    "        s = self.shortcut(x)\n",
    "        h = self.block1(x)\n",
    "        h = self.block2(h)\n",
    "        return h + s\n",
    "\n",
    "\n",
    "# GANジェネレータ用のアップサンプリング層\n",
    "# 最近傍補間で特徴マップの縦幅・横幅を 2 倍に拡大したのち，Residual Block を適用する\n",
    "# この方法は逆畳込み（nn.ConvTranspose2d）の代わりとして使用でき，かつ，逆畳込みより checker board artifact が生じにくいと言われている\n",
    "class myUpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myUpsamplingBlock, self).__init__()\n",
    "        self.up = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.rb = ResBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, sn=False)\n",
    "    def forward(self, x):\n",
    "        h = self.up(x)\n",
    "        return self.rb(h)\n",
    "\n",
    "\n",
    "# GANディスクリミネータ用のダウンサンプリング層\n",
    "# Residual Block を適用したのち，average pooling を実行することにより特徴マップの縦幅・横幅を 1/2 に縮小する\n",
    "class myDownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myDownsamplingBlock, self).__init__()\n",
    "        self.rb = ResBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, sn=True)\n",
    "        self.down = nn.AvgPool2d(kernel_size=2)\n",
    "    def forward(self, x):\n",
    "        h = self.rb(x)\n",
    "        return self.down(h)\n",
    "\n",
    "\n",
    "# 顔画像生成ニューラルネットワーク\n",
    "# GAN生成器（ジェネレータ）のサンプル\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # C: 出力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 出力顔画像の縦幅（32の倍数と仮定）\n",
    "    # W: 出力顔画像の横幅（32の倍数と仮定）\n",
    "    # N: 入力の特徴ベクトル（乱数ベクトル）の次元数\n",
    "    def __init__(self, C, H, W, N):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # 前処理のための層\n",
    "        # 入力の特徴ベクトルを，チャンネル数 512, 縦幅 H/32, 横幅 W/32 の特徴マップに変換するために使用\n",
    "        self.conv0 = nn.Sequential(\n",
    "            Reshape(size=(N, 1, 1)),\n",
    "            nn.ConvTranspose2d(in_channels=N, out_channels=512, kernel_size=(H//32, W//32), stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # アップサンプリング層1～5\n",
    "        # これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 2 倍になる\n",
    "        # 5つ通すことになるので，最終的には都合 32 倍になる -> ゆえに縦幅 H/32, 横幅 W/32 の特徴マップからスタートする\n",
    "        self.up1 = myUpsamplingBlock(in_channels=512, out_channels=256)\n",
    "        self.up2 = myUpsamplingBlock(in_channels=256, out_channels=128)\n",
    "        self.up3 = myUpsamplingBlock(in_channels=128, out_channels=64)\n",
    "        self.up4 = myUpsamplingBlock(in_channels=64, out_channels=32)\n",
    "        self.up5 = myUpsamplingBlock(in_channels=32, out_channels=32)\n",
    "\n",
    "        # 出力画像生成用の畳込み層\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=C, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.conv0(z) # N 次元の特徴ベクトルをチャンネル数 512, 縦幅 H/32, 横幅 W/32 の特徴マップに変換\n",
    "        h = self.up1(h)\n",
    "        h = self.up2(h)\n",
    "        h = self.up3(h)\n",
    "        h = self.up4(h)\n",
    "        h = self.up5(h)\n",
    "        y = torch.tanh(self.conv5(h))\n",
    "        return y\n",
    "\n",
    "\n",
    "# 顔画像が Real か Fake を判定するニューラルネットワーク\n",
    "# GAN識別器（ディスクリミネータ）のサンプル\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    # C: 入力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 入力顔画像の縦幅（32の倍数と仮定）\n",
    "    # W: 入力顔画像の横幅（32の倍数と仮定）\n",
    "    def __init__(self, C, H, W):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 訓練データ量の不足を補うためのデータ拡張（Data Augmentation）処理\n",
    "        # 詳しくは参考サイト（ https://qiita.com/T-STAR/items/e079da240d886bbb4ed0 など ）を参照\n",
    "        self.preprocess = DiscriminatorAugmentation(H, W, p_hflip=0.5, p_vflip=0.4, p_rot=0.4) # 確率0.5で左右反転，確率0.4で上下反転，確率0.4で回転\n",
    "\n",
    "        # ダウンサンプリング層1～5\n",
    "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 1/2 になる\n",
    "        self.down1 = myDownsamplingBlock(in_channels=C, out_channels=32)\n",
    "        self.down2 = myDownsamplingBlock(in_channels=32, out_channels=64)\n",
    "        self.down3 = myDownsamplingBlock(in_channels=64, out_channels=128)\n",
    "        self.down4 = myDownsamplingBlock(in_channels=128, out_channels=256)\n",
    "        self.down5 = myDownsamplingBlock(in_channels=256, out_channels=256)\n",
    "\n",
    "        # 平坦化\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # 全結合層1（spectral normalization を使用）\n",
    "        # ダウンサンプリング層1～5を通すことにより特徴マップの縦幅・横幅は都合 1/32 になっているので，\n",
    "        # 入力側のパーセプトロン数は 256*(H/32)*(W/32) = H*W/4\n",
    "        self.fc1 = nn.utils.spectral_norm(nn.Linear(in_features=H*W//4, out_features=256))\n",
    "\n",
    "        # 全結合層2\n",
    "        # in_features の設定値については，次の Minibatch Discrimination に関するコメントを参照\n",
    "        self.fc2 = nn.Linear(in_features=384, out_features=1)\n",
    "\n",
    "        # Minibatch Discrimination: モード崩壊を回避するための技法の一つ\n",
    "        # 詳しくは参考サイト（ http://www2.media.is.uec.ac.jp/column/20210507_ni など ）を参照\n",
    "        # [使用方法] - in_features には直前の層の out_features と同じ値を設定する\n",
    "        #            - out_features の設定は自由，ただし，in_features と out_features の和が直後の層の in_features に一致するようにする\n",
    "        #            - このサンプルコードでは，md.in_features == fc1.out_features == 256\n",
    "        #              かつ fc2.in_features == md.in_features + md.out_features == 256 + 128 == 384\n",
    "        self.md = MinibatchDiscrimination(in_features=256, out_features=128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 本来であれば，ディスクリミネータの出力が 0～1 の範囲となるよう，最終層の活性化関数として sigmoid を適用すべきであるが，\n",
    "        # このサンプルコードでは損失関数側で sigmoid 適用することになるので, ここでは最終層で活性化関数を適用しない\n",
    "        h = self.preprocess(x)\n",
    "        h = self.down1(h)\n",
    "        h = self.down2(h)\n",
    "        h = self.down3(h)\n",
    "        h = self.down4(h)\n",
    "        h = self.down5(h)\n",
    "        h = self.flat(h)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.md(h) # Minibatch Discrimination\n",
    "        z = self.fc2(h) # 上記の通り，最終層では活性化関数なし\n",
    "        return z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "from mylib.utility import save_datasets, load_datasets_from_file\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "    train_dataset, _ = load_datasets_from_file(MODEL_DIR)\n",
    "    if train_dataset is None:\n",
    "        print('error: there is no checkpoint previously saved.')\n",
    "        exit()\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # CSVファイルを読み込み, 訓練データセットを用意\n",
    "    # 今回は，全てのデータを学習用に回す\n",
    "    train_dataset = CSVBasedDataset(\n",
    "        filename = DATASET_CSV,\n",
    "        items = [\n",
    "            'File Path', # X\n",
    "        ],\n",
    "        dtypes = [\n",
    "            'image', # Xの型\n",
    "        ],\n",
    "        dirname = DATA_DIR,\n",
    "        img_transform = transforms.CenterCrop((H, W)), # 処理量を少しでも抑えるため，画像中央の H×W ピクセルの部分だけを対象とする\n",
    "    )\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    save_datasets(MODEL_DIR, train_dataset)\n",
    "\n",
    "# 訓練データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習処理の実行\n",
    "- GANの学習は一般に安定せず，最終的なモデルよりも学習途中のモデルの方が優れていることがよくあります\n",
    "- このため，エポックごとにモデル保存処理を実行し，学習終了後，最良（と思われる）モデルをロードして利用することも多いです\n",
    "- ただし，これを Paperspace Gradient などのクラウド環境で実行するとストレージ使用量の上限（Paperspace Gradient では 5GB）を超えてしまう可能性があるので，注意してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.loss_functions import GANLoss\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images, to_sigmoid_image, to_tanh_image, autosaved_model_name\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "# 何エポックに1回の割合で学習経過を表示するか（モデル保存処理もこれと同じ頻度で実行）\n",
    "INTERVAL_FOR_SHOWING_PROGRESS = 10\n",
    "\n",
    "# spectral normalization の使用によりディスクリミネータが弱体化するので，ジェネレータの更新回数を減らすことが望ましい（らしいが，実際にはなんとも言い難い）\n",
    "# ここでは，ジェネレータを5回に1回の割合でしか更新しないことにする\n",
    "N_DIS = 5 # この値を 1 にすれば，ジェネレータも毎回更新されるようになる\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "gen_model = Generator(C=C, H=H, W=W, N=N).to(DEVICE)\n",
    "dis_model = Discriminator(C=C, H=H, W=W).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "gen_optimizer = optim.Adam(gen_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "dis_optimizer = optim.Adam(dis_model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, gen_model, gen_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_GEN_MODEL, CHECKPOINT_GEN_OPT, N_EPOCHS, gen_model, gen_optimizer)\n",
    "    _, _, dis_model, dis_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DIS_MODEL, CHECKPOINT_DIS_OPT, N_EPOCHS, dis_model, dis_optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "# Label Smooting（GANの学習を安定化させる技法の一つ）を採用\n",
    "# 詳しくは参考サイト（ https://tatsy.github.io/programming-for-beginners/python/stabilize-gan-training/ など ）を参照\n",
    "loss_func = GANLoss(label_smoothing=True)\n",
    "\n",
    "# 検証の際に使用する乱数ベクトルを用意\n",
    "Z_valid = torch.randn((BATCH_SIZE, N)).to(DEVICE)\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['G loss', 'D loss'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    gen_model.train()\n",
    "    dis_model.train()\n",
    "    sum_gen_loss = 0\n",
    "    sum_dis_loss = 0\n",
    "    n_iter = 1 # 1エポック内でのループ回数を記録する変数（ジェネレータの更新回数を制御するために使用）\n",
    "    for X in tqdm(train_dataloader):\n",
    "        for param in gen_model.parameters():\n",
    "            param.grad = None\n",
    "        for param in dis_model.parameters():\n",
    "            param.grad = None\n",
    "        Z = torch.randn((len(X), N)).to(DEVICE) # 乱数ベクトルを用意（通常の標準正規分布から作成）\n",
    "        real = to_tanh_image(X).to(DEVICE) # Real画像を用意（to_tanh_image 関数を用い，画素値の範囲が -1〜1 となるように調整しておく）\n",
    "        fake = gen_model(Z) # Fake画像を生成（2行上で用意した Z から生成）\n",
    "        fake_cpy = fake.detach() # Fake画像のコピーを用意しておく\n",
    "        ### ジェネレータの学習 ###\n",
    "        if n_iter % N_DIS == 0:\n",
    "            Y_fake = dis_model(fake) # Fake画像を識別\n",
    "            gen_loss = loss_func.G_loss(Y_fake)\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "            sum_gen_loss += float(gen_loss) * len(X)\n",
    "        ### ディスクリミネータの学習 ###\n",
    "        for param in dis_model.parameters():\n",
    "            param.grad = None # ジェネレータの学習時の計算した勾配を一旦リセット\n",
    "        Y_real = dis_model(real) # Real画像を識別\n",
    "        Y_fake = dis_model(fake_cpy) # Fake画像を識別（コピー変数の方を使用）\n",
    "        dis_loss = loss_func.D_loss(Y_fake, as_real=False) + loss_func.D_loss(Y_real, as_real=True)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        sum_dis_loss += float(dis_loss) * len(X)\n",
    "        n_iter += 1\n",
    "    avg_gen_loss = sum_gen_loss * N_DIS / train_size\n",
    "    avg_dis_loss = sum_dis_loss / train_size\n",
    "    loss_viz.add_value('G loss', avg_gen_loss) # 訓練データに対する損失関数の値を記録\n",
    "    loss_viz.add_value('D loss', avg_dis_loss) # 同上\n",
    "    print('generator train loss = {0:.6f}'.format(avg_gen_loss))\n",
    "    print('discriminator train loss = {0:.6f}'.format(avg_dis_loss))\n",
    "    print('')\n",
    "\n",
    "    # 検証（学習経過の表示，モデル自動保存）\n",
    "    if epoch == 0 or (epoch + 1) % INTERVAL_FOR_SHOWING_PROGRESS == 0:\n",
    "        gen_model.eval()\n",
    "        dis_model.eval()\n",
    "        if epoch == 0:\n",
    "            real = to_sigmoid_image(real) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "            show_images(real.to('cpu').detach(), num=32, num_per_row=8, title='real images', save_fig=False, save_dir=MODEL_DIR) # Real画像の例を表示（最初のエポックのみ）\n",
    "        with torch.inference_mode():\n",
    "            fake = gen_model(Z_valid) # 事前に用意しておいた検証用乱数からFake画像を生成\n",
    "            #fake = gen_model(torch.randn((BATCH_SIZE, N)).to(DEVICE)) # エポックごとに異なる乱数を使用する場合はこのようにする\n",
    "        fake = to_sigmoid_image(fake) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "        show_images(fake.to('cpu').detach(), num=32, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR) # 現在のジェネレータによるFake画像の例を表示\n",
    "        torch.save(gen_model.state_dict(), autosaved_model_name(MODEL_FILE_G, epoch + 1)) # 学習途中のモデルを保存したい場合はこのようにする\n",
    "\n",
    "    # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_GEN_MODEL, CHECKPOINT_GEN_OPT, epoch+1, gen_model, gen_optimizer)\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DIS_MODEL, CHECKPOINT_DIS_OPT, epoch+1, dis_model, dis_optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "gen_model = gen_model.to('cpu')\n",
    "dis_model = dis_model.to('cpu')\n",
    "torch.save(gen_model.state_dict(), MODEL_FILE_G)\n",
    "#torch.save(dis_model.state_dict(), MODEL_FILE_D) # ディスクリミネータも保存したい場合はこのようにする\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "gen_model = Generator(C=C, H=H, W=W, N=N).to(DEVICE)\n",
    "gen_model.load_state_dict(torch.load(MODEL_FILE_G)) # 最終モデルをロードする場合\n",
    "#gen_model.load_state_dict(torch.load(autosaved_model_name(MODEL_FILE_G, 80))) # 例えば80エポック目のモデルをロードしたい場合は，このようにする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理（正規分布に従ってランダムサンプリングした乱数をデコーダに通して画像を生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "gen_model = gen_model.to(DEVICE)\n",
    "gen_model.eval()\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 32\n",
    "\n",
    "# 標準正規分布 N(0,1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((n_gen, N)).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    Y = gen_model(Z)\n",
    "    Y = to_sigmoid_image(Y)\n",
    "    show_images(Y.to('cpu').detach(), num=n_gen, num_per_row=8, title='GAN_sample_generated', save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
