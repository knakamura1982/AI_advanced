{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "# 再開モードの場合も, このエポック数の分だけ追加学習される（N_EPOCHSは最終エポック番号ではない）\n",
    "N_EPOCHS_FOR_VAE = 300\n",
    "N_EPOCHS_FOR_UNET = 1200\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "DATASET_CSV = './tinyCelebA/image_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './tinyCelebA/'\n",
    "\n",
    "# 画像サイズ\n",
    "H = 128 # 縦幅\n",
    "W = 128 # 横幅\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 潜在特徴マップのチャンネル数\n",
    "ZC = 4\n",
    "ZH = H // 4 # 本プログラムでは, 潜在特徴マップの縦幅・横幅は入力画像の1/4となるようにVAEモデルを設計している\n",
    "ZW = W // 4 # 同上\n",
    "\n",
    "# 拡散過程／逆拡散過程（生成過程）のタイムステップ数\n",
    "N_TIMESTEPS = 1000\n",
    "\n",
    "# DDIMを用いて, より短時間で生成過程を実行する場合のタイムステップ数\n",
    "N_GEN_TIMESTEPS = 20\n",
    "\n",
    "# タイムステップ情報を何次元のベクトルにエンコードするか\n",
    "TIME_EMBED_DIM = 512\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './LDDPM_models/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "VAE_MODEL_FILE = os.path.join(MODEL_DIR, './vae_model.pth')\n",
    "UNET_MODEL_FILE = os.path.join(MODEL_DIR, './unet_model.pth')\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイル\n",
    "VAE_CHECKPOINT_EPOCH = os.path.join(MODEL_DIR, 'vae_checkpoint_epoch.pkl')\n",
    "VAE_CHECKPOINT_MODEL = os.path.join(MODEL_DIR, 'vae_checkpoint_model.pth')\n",
    "VAE_CHECKPOINT_OPT = os.path.join(MODEL_DIR, 'vae_checkpoint_opt.pth')\n",
    "UNET_CHECKPOINT_EPOCH = os.path.join(MODEL_DIR, 'unet_checkpoint_epoch.pkl')\n",
    "UNET_CHECKPOINT_MODEL = os.path.join(MODEL_DIR, 'unet_checkpoint_model.pth')\n",
    "UNET_CHECKPOINT_OPT = os.path.join(MODEL_DIR, 'unet_checkpoint_opt.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# タイムステップ情報の埋め込みベクトルを計算する層\n",
    "#   - time_embed_dim: タイムステップ情報埋め込みベクトルの次元数\n",
    "class SinusoidalTimeEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, time_embed_dim):\n",
    "        super(SinusoidalTimeEmbeddings, self).__init__()\n",
    "        self.embed_dim = time_embed_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.embed_dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000, device=t.device)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=t.device) * -embeddings)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# 先に Group Normalization + Siwsh を適用してから畳み込み処理を実行する畳み込み層\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "class PreNormConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_groups, kernel_size, stride, padding, init_scale=1.0):\n",
    "        super(PreNormConv2d, self).__init__()\n",
    "        self.act = nn.SiLU()\n",
    "        self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        nn.init.xavier_uniform_(self.conv.weight, gain=math.sqrt(init_scale or 1e-10))\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm(x)\n",
    "        h = self.act(h)\n",
    "        return self.conv(h)\n",
    "\n",
    "\n",
    "# タイムステップ情報を考慮した ResBlock\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "#   - time_embed_dim: タイムステップ情報埋め込みベクトルの次元数（0以下の場合は通常の ResBlock になる）\n",
    "class DDPMResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_groups, kernel_size=3, time_embed_dim=0):\n",
    "        super(DDPMResBlock, self).__init__()\n",
    "        if time_embed_dim > 0:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.SiLU(), \n",
    "                nn.Linear(time_embed_dim, out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.mlp = None\n",
    "        self.block1 = PreNormConv2d(in_channels, out_channels, num_groups=num_groups, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
    "        self.block2 = PreNormConv2d(out_channels, out_channels, num_groups=num_groups, kernel_size=kernel_size, stride=1, padding=kernel_size//2, init_scale=0.0)\n",
    "        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, time_embedding=None):\n",
    "        h = self.block1(x)\n",
    "        if self.mlp is not None:\n",
    "            h = h + self.mlp(time_embedding).unsqueeze(2).unsqueeze(3)\n",
    "        h = self.block2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "\n",
    "# Linear Attention\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "#   - num_heads: マルチヘッドアテンションのヘッド数\n",
    "#   - embed_dim: 1ヘッドあたりの次元数（タイムステップ情報埋め込みベクトルの次元数とは別）\n",
    "class DDPMLinearAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_groups, num_heads, embed_dim):\n",
    "        super(DDPMLinearAttention, self).__init__()\n",
    "        self.scale = embed_dim ** (- 0.5)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)\n",
    "        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.to_qkv = nn.Conv2d(in_channels, num_heads * embed_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(num_heads * embed_dim, out_channels, 1)\n",
    "        nn.init.xavier_uniform_(self.to_out.weight, gain=1e-5)\n",
    "\n",
    "    # x: 特徴マップ（バッチサイズ, チャンネル数, 縦幅, 横幅の4次元テンソル）\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.size()\n",
    "        q, k, v = self.to_qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        q = torch.reshape(q, (B, self.num_heads, self.embed_dim, H * W))\n",
    "        k = torch.reshape(k, (B, self.num_heads, self.embed_dim, H * W))\n",
    "        v = torch.reshape(v, (B, self.num_heads, self.embed_dim, H * W))\n",
    "        q = q.softmax(dim=-2) * self.scale\n",
    "        k = k.softmax(dim=-1)\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v).contiguous()\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q).contiguous()\n",
    "        out = torch.reshape(out, (B, self.num_heads * self.embed_dim, H, W))\n",
    "        return self.to_out(out) + self.skip(x)\n",
    "\n",
    "\n",
    "# 通常の Multi-head Attention\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "#   - num_heads: マルチヘッドアテンションのヘッド数\n",
    "#   - embed_dim: 1ヘッドあたりの次元数（タイムステップ情報埋め込みベクトルの次元数とは別）\n",
    "class DDPMAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_groups, num_heads, embed_dim):\n",
    "        super(DDPMAttention, self).__init__()\n",
    "        self.scale = embed_dim ** (- 0.5)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)\n",
    "        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.to_qkv = nn.Conv2d(in_channels, num_heads * embed_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(num_heads * embed_dim, out_channels, 1)\n",
    "        nn.init.xavier_uniform_(self.to_out.weight, gain=1e-5)\n",
    "\n",
    "    # x: 特徴マップ（バッチサイズ, チャンネル数, 縦幅, 横幅の4次元テンソル）\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.size()\n",
    "        q, k, v = self.to_qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        q = torch.reshape(q, (B, self.num_heads, self.embed_dim, H * W))\n",
    "        k = torch.reshape(k, (B, self.num_heads, self.embed_dim, H * W))\n",
    "        v = torch.reshape(v, (B, self.num_heads, self.embed_dim, H * W))\n",
    "        q = q * self.scale\n",
    "        sim = torch.einsum(\"b h d i, b h d j -> b h i j\", q, k).contiguous()\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum(\"b h i j, b h d j -> b h i d\", attn, v).contiguous()\n",
    "        out = torch.reshape(out.permute(0, 1, 3, 2), (B, self.num_heads * self.embed_dim, H, W))\n",
    "        return self.to_out(out) + self.skip(x)\n",
    "\n",
    "\n",
    "# アテンションの種類を選択する関数\n",
    "def get_attention_block(attention_type, in_channels, out_channels, num_groups, num_heads, embed_dim):\n",
    "    if attention_type == 'linear':\n",
    "        attn = DDPMLinearAttention(in_channels=in_channels, out_channels=out_channels, num_groups=num_groups, num_heads=num_heads, embed_dim=embed_dim)\n",
    "    elif attention_type == 'normal':\n",
    "        attn = DDPMAttention(in_channels=in_channels, out_channels=out_channels, num_groups=num_groups, num_heads=num_heads, embed_dim=embed_dim)\n",
    "    elif attention_type == 'none':\n",
    "        attn = nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    return attn\n",
    "\n",
    "\n",
    "# 拡散モデルを実現する U-Net の中間層\n",
    "#   - time_embed_dim: タイムステップ情報埋め込みベクトルの次元数\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "#   - num_heads: マルチヘッドアテンションのヘッド数（1ヘッドあたりの次元数は channels/num_heads で指定）\n",
    "#   - attention_type: 'normal'なら通常のマルチヘッドアテンション, 'linear'なら linear attention が使用される. 'none'の場合はアテンションなし\n",
    "class DDPMMiddleLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, time_embed_dim, num_groups, num_heads=8, attention_type='none'):\n",
    "        super(DDPMMiddleLayer, self).__init__()\n",
    "        embed_dim = channels // num_heads\n",
    "        self.block1 = DDPMResBlock(in_channels=channels, out_channels=channels, num_groups=num_groups, kernel_size=3, time_embed_dim=time_embed_dim)\n",
    "        self.block2 = DDPMResBlock(in_channels=channels, out_channels=channels, num_groups=num_groups, kernel_size=3, time_embed_dim=time_embed_dim)\n",
    "        self.attn = get_attention_block(attention_type, channels, channels, num_groups, num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, x, time_embedding=None):\n",
    "        h = self.block1(x, time_embedding)\n",
    "        h = self.attn(h)\n",
    "        y = self.block2(h, time_embedding)\n",
    "        return y\n",
    "\n",
    "\n",
    "# 拡散モデルを実現する U-Net のダウンサンプリング層\n",
    "#   - time_embed_dim: タイムステップ情報埋め込みベクトルの次元数\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "#   - num_heads: マルチヘッドアテンションのヘッド数（1ヘッドあたりの次元数は in_channels/num_heads で指定）\n",
    "#   - attention_type: 'normal'なら通常のマルチヘッドアテンション, 'linear'なら linear attention が使用される. 'none'の場合はアテンションなし\n",
    "#   - with_downsample: Falseの場合はダウンサンプリングを実行しない\n",
    "#   - with_skip_output: Falseの場合はスキップ接続用の特徴量を出力しない\n",
    "class DDPMDownSamplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, time_embed_dim, num_groups, num_heads=8, attention_type='none', with_downsample=True, with_skip_output=True):\n",
    "        super(DDPMDownSamplingLayer, self).__init__()\n",
    "        embed_dim = in_channels // num_heads\n",
    "        self.with_skip_output = with_skip_output\n",
    "        self.block1 = DDPMResBlock(in_channels=in_channels, out_channels=in_channels, num_groups=num_groups, kernel_size=3, time_embed_dim=time_embed_dim)\n",
    "        self.block2 = DDPMResBlock(in_channels=in_channels, out_channels=in_channels, num_groups=num_groups, kernel_size=3, time_embed_dim=time_embed_dim)\n",
    "        self.attn = get_attention_block(attention_type, in_channels, in_channels, num_groups, num_heads, embed_dim)\n",
    "        if with_downsample:\n",
    "            self.down = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.down = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, time_embedding=None):\n",
    "        h = self.block1(x, time_embedding)\n",
    "        s = self.attn(h) # このブロックの出力をアップサンプリング層へのスキップ接続として使用\n",
    "        h = self.block2(s, time_embedding)\n",
    "        y = self.down(h)\n",
    "        if self.with_skip_output:\n",
    "            return s, y\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "\n",
    "# 拡散モデルを実現する U-Net のアップサンプリング層\n",
    "#   - time_embed_dim: タイムステップ情報埋め込みベクトルの次元数\n",
    "#   - num_groups: Group Nromalization におけるグループ数\n",
    "#   - num_heads: マルチヘッドアテンションのヘッド数（1ヘッドあたりの次元数は out_channels/num_heads で指定）\n",
    "#   - attention_type: 'normal'なら通常のマルチヘッドアテンション, 'linear'なら linear attention が使用される. 'none'の場合はアテンションなし\n",
    "#   - with_upsample: Falseの場合はダウンサンプリングを実行しない\n",
    "#   - with_skip_input: Falseの場合はスキップ接続用の特徴量を受け付けない\n",
    "class DDPMUpSamplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, time_embed_dim, num_groups, num_heads=8, attention_type='none', with_upsample=True, with_skip_input=True):\n",
    "        super(DDPMUpSamplingLayer, self).__init__()\n",
    "        embed_dim = out_channels // num_heads\n",
    "        block1_out_channels = out_channels * 2 if with_skip_input else out_channels\n",
    "        self.block1 = DDPMResBlock(in_channels=block1_out_channels, out_channels=out_channels, num_groups=num_groups, kernel_size=3, time_embed_dim=time_embed_dim)\n",
    "        self.block2 = DDPMResBlock(in_channels=out_channels, out_channels=out_channels, num_groups=num_groups, kernel_size=3, time_embed_dim=time_embed_dim)\n",
    "        self.attn = get_attention_block(attention_type, out_channels, out_channels, num_groups, num_heads, embed_dim)\n",
    "        if with_upsample:\n",
    "            self.up = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.up = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, s=None, time_embedding=None):\n",
    "        h = self.up(x)\n",
    "        if s is not None:\n",
    "            h = torch.cat((h, s), dim=1)\n",
    "        h = self.block1(h, time_embedding)\n",
    "        h = self.attn(h)\n",
    "        y = self.block2(h, time_embedding)\n",
    "        return y\n",
    "\n",
    "\n",
    "# 潜在拡散モデルを実現するためのVAE\n",
    "# エンコーダはサイズ (C, H, W) の画像をサイズ (ZC, H/4, W/4) の潜在特徴マップに変換\n",
    "# デコーダはサイズ (ZC, H/4, W/4) の潜在特徴マップをサイズ (C, H, W) の画像に変換\n",
    "class LDDPM_VAE(nn.Module):\n",
    "\n",
    "    # C: 入力画像のチャンネル数（グレースケール画像なら1，カラー画像なら3）\n",
    "    # ZC: 潜在特徴マップのチャンネル数\n",
    "    # num_groups: Group Nromalization におけるグループ数\n",
    "    def __init__(self, C, ZC, num_groups=16):\n",
    "        super(LDDPM_VAE, self).__init__()\n",
    "\n",
    "        # 層ごとのチャンネル数\n",
    "        L1_C = 64\n",
    "        L2_C = 128\n",
    "        L3_C = 256\n",
    "        L4_C = 256\n",
    "\n",
    "        # エンコーダ側\n",
    "        self.init_conv = nn.Conv2d(in_channels=C, out_channels=L1_C, kernel_size=1, stride=1, padding=0)\n",
    "        self.downsample = nn.Sequential(\n",
    "            DDPMDownSamplingLayer(in_channels=L1_C, out_channels=L2_C, time_embed_dim=0, num_groups=num_groups, with_skip_output=False),\n",
    "            DDPMDownSamplingLayer(in_channels=L2_C, out_channels=L3_C, time_embed_dim=0, num_groups=num_groups, with_skip_output=False),\n",
    "            DDPMDownSamplingLayer(in_channels=L3_C, out_channels=L4_C, time_embed_dim=0, num_groups=num_groups, with_skip_output=False, with_downsample=False),\n",
    "            PreNormConv2d(in_channels=L4_C, out_channels=ZC*2, num_groups=num_groups, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "        self.to_mu = nn.Conv2d(in_channels=ZC*2, out_channels=ZC, kernel_size=1, stride=1, padding=0)\n",
    "        self.to_lnvar = nn.Conv2d(in_channels=ZC*2, out_channels=ZC, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # デコーダ側\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ZC, out_channels=L4_C, kernel_size=1, stride=1, padding=0),\n",
    "            DDPMUpSamplingLayer(in_channels=L4_C, out_channels=L3_C, time_embed_dim=0, num_groups=num_groups, with_skip_input=False, with_upsample=False),\n",
    "            DDPMUpSamplingLayer(in_channels=L3_C, out_channels=L2_C, time_embed_dim=0, num_groups=num_groups, with_skip_input=False),\n",
    "            DDPMUpSamplingLayer(in_channels=L2_C, out_channels=L1_C, time_embed_dim=0, num_groups=num_groups, with_skip_input=False),\n",
    "        )\n",
    "        self.last_conv = nn.Sequential(\n",
    "            PreNormConv2d(in_channels=L1_C, out_channels=C, num_groups=num_groups, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    # エンコード\n",
    "    def encode(self, x, testmode=False):\n",
    "        h = self.init_conv(x)\n",
    "        h = self.downsample(h)\n",
    "        mu = self.to_mu(h)\n",
    "        if testmode:\n",
    "            return mu # テスト時は乱数を付加する前の mu だけを返す\n",
    "        else:\n",
    "            lnvar = self.to_lnvar(h)\n",
    "            eps = torch.randn_like(mu)\n",
    "            z = mu + eps * torch.exp(0.5 * lnvar)\n",
    "            return z, mu, lnvar\n",
    "\n",
    "    # デコード\n",
    "    def decode(self, z):\n",
    "        h = self.upsample(z)\n",
    "        y = self.last_conv(h)\n",
    "        return y\n",
    "\n",
    "    # 再構成\n",
    "    def forward(self, x, testmode=False):\n",
    "        if testmode:\n",
    "            mu = self.encode(x, testmode=True)\n",
    "            return self.decode(mu) # テスト時は乱数を付加する前の mu をデコーダに入力する\n",
    "        else:\n",
    "            z, mu, lnvar = self.encode(x, testmode=False)\n",
    "            y = self.decode(z)\n",
    "            return y, mu, lnvar\n",
    "\n",
    "\n",
    "# 潜在拡散モデルを実現するU-Net\n",
    "class LDDPM_UNet(nn.Module):\n",
    "\n",
    "    # ZC: VAEによりエンコードされた特徴マップのチャンネル数\n",
    "    # time_embed_dim: タイムステップ情報をエンコーディングする際のコードベクトルの次元数（偶数）\n",
    "    # num_groups: Group Nromalization におけるグループ数\n",
    "    def __init__(self, ZC, time_embed_dim, num_groups=16):\n",
    "        super(LDDPM_UNet, self).__init__()\n",
    "\n",
    "        # 層ごとのチャンネル数\n",
    "        L1_C = 128\n",
    "        L2_C = 256\n",
    "        L3_C = 512\n",
    "        L4_C = 512\n",
    "\n",
    "        # タイムステップ情報のエンコーディングを担当する層\n",
    "        self.time_encoder = nn.Sequential(\n",
    "            SinusoidalTimeEmbeddings(time_embed_dim),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        # 入力画像に対し最初に適用する畳み込み層\n",
    "        self.init_conv = nn.Conv2d(in_channels=ZC, out_channels=L1_C, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # U-Netのダウンサンプリング層\n",
    "        self.down1 = DDPMDownSamplingLayer(in_channels=L1_C, out_channels=L2_C, time_embed_dim=time_embed_dim, num_groups=num_groups)\n",
    "        self.down2 = DDPMDownSamplingLayer(in_channels=L2_C, out_channels=L3_C, time_embed_dim=time_embed_dim, num_groups=num_groups)\n",
    "        self.down3 = DDPMDownSamplingLayer(in_channels=L3_C, out_channels=L4_C, time_embed_dim=time_embed_dim, num_groups=num_groups, attention_type='linear')\n",
    "\n",
    "        # U-Netの中間層\n",
    "        self.mid = DDPMMiddleLayer(channels=L4_C, time_embed_dim=time_embed_dim, num_groups=num_groups, attention_type='linear')\n",
    "\n",
    "        # U-Netのアップサンプリング層\n",
    "        self.up3 = DDPMUpSamplingLayer(in_channels=L4_C, out_channels=L3_C, time_embed_dim=time_embed_dim, num_groups=num_groups, attention_type='linear')\n",
    "        self.up2 = DDPMUpSamplingLayer(in_channels=L3_C, out_channels=L2_C, time_embed_dim=time_embed_dim, num_groups=num_groups)\n",
    "        self.up1 = DDPMUpSamplingLayer(in_channels=L2_C, out_channels=L1_C, time_embed_dim=time_embed_dim, num_groups=num_groups)\n",
    "\n",
    "        # 最後に実行する畳み込み層\n",
    "        self.last_conv = PreNormConv2d(in_channels=L1_C, out_channels=ZC, num_groups=num_groups, kernel_size=1, stride=1, padding=0, init_scale=0.0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        h = self.init_conv(x) # 最初の畳み込み\n",
    "        time_embedding = self.time_encoder(t) # タイムステップ情報のエンコーディング\n",
    "        s1, h = self.down1(h, time_embedding) # ダウンサンプリング層（ s1～s3 はアップサンプリング層へのスキップ接続として使用）\n",
    "        s2, h = self.down2(h, time_embedding)\n",
    "        s3, h = self.down3(h, time_embedding)\n",
    "        h = self.mid(h, time_embedding) # 中間層\n",
    "        h = self.up3(h, s3, time_embedding) # アップサンプリング層\n",
    "        h = self.up2(h, s2, time_embedding)\n",
    "        h = self.up1(h, s1, time_embedding)\n",
    "        y = self.last_conv(h) # 最終畳み込み\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ノイズスケジューリングの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ノイズスケジューラ\n",
    "class NoiseScheduler:\n",
    "\n",
    "    def __init__(self, device, method:str='linear', num_timesteps:int=1000, start:float=0.0001, end:float=0.02, s:float=0.008, clip:float=0.999):\n",
    "\n",
    "        # beta を用意\n",
    "        if method == 'cosine': # あまり上手く動かない．実装ミス？\n",
    "            num_timesteps += 1\n",
    "            T = num_timesteps - 1\n",
    "            t = torch.arange(0, num_timesteps)\n",
    "            alpha_bar = torch.cos(0.5 * torch.pi * ((t/T)+s)/(1+s))**2\n",
    "            alpha_bar = alpha_bar / alpha_bar[0]\n",
    "            beta = torch.clamp(1.0 - alpha_bar[1:] / alpha_bar[:-1], max=clip)\n",
    "        elif method == 'quadratic': # 十分なエポック数を試したことがない\n",
    "            beta = torch.linspace(start**0.5, end**0.5, num_timesteps)**2\n",
    "        elif method == 'sigmoid': # 一回も試したことがない\n",
    "            beta = torch.sigmoid(torch.linspace(-6, 6, num_timesteps)) * (end - start) + start\n",
    "        elif method == 'linear': # 結局これが無難？\n",
    "            beta = torch.linspace(start, end, num_timesteps)\n",
    "        else:\n",
    "            raise NotImplementedError(method)\n",
    "        self.beta = beta.to(device)\n",
    "\n",
    "        # alpha, alpha_bar などを用意\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, axis=0)\n",
    "        self.alpha_bar_prev = F.pad(self.alpha_bar[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
    "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - self.alpha_bar)\n",
    "        self.sqrt_inv_alpha = torch.sqrt(1.0 / self.alpha)\n",
    "\n",
    "        # 逆拡散過程実行時に使用する係数を用意\n",
    "        self.var_coeff = torch.sqrt(self.beta * (1.0 - self.alpha_bar_prev) / (1.0 - self.alpha_bar))\n",
    "        self.noise_scale_coeff = self.sqrt_inv_alpha * self.beta / self.sqrt_one_minus_alpha_bar\n",
    "\n",
    "    # タイプステップ t において x0 に正規乱数ノイズを付加したデータを生成\n",
    "    #   - x0: ノイズ付加前の入力画像（ミニバッチ形式で与える）\n",
    "    #   - t: タイムステップ（ミニバッチ形式で与える）\n",
    "    #   - noise: 標準正規分布に従うシードノイズ（Noneの場合は関数内で生成）\n",
    "    def get_noisy_sample(self, x0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        return self.sqrt_alpha_bar[t].reshape(-1, 1, 1, 1) * x0 + self.sqrt_one_minus_alpha_bar[t].reshape(-1, 1, 1, 1) * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 逆拡散過程（生成過程）を実行する関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "# DDIMによるデータ生成\n",
    "# こちらは, 学習時よりも少ないタイムステップ数で簡易的に画像を生成したい場合に使用\n",
    "def LDDIM_generate(Z, model, vae_model, noise_scheduler, n_timesteps, n_gen_timesteps, show_progress=False):\n",
    "\n",
    "    t_list = np.round(np.linspace(0, n_timesteps-1, n_gen_timesteps)).astype(np.int32)\n",
    "    s_list = np.concatenate([[0], t_list[:-1]])\n",
    "    timesteps = np.concatenate([t_list.reshape(-1, 1), s_list.reshape(-1, 1)], axis=1)\n",
    "    with torch.no_grad():\n",
    "        for t_idx, s_idx in tqdm(reversed(timesteps), total=n_gen_timesteps):\n",
    "\n",
    "            # ノイズ推定\n",
    "            t = t_idx * torch.ones((len(Z),), device=Z.device).long()\n",
    "            noise = model(Z, t)\n",
    "\n",
    "            # ノイズ除去\n",
    "            if t_idx == 0:\n",
    "                Z = noise_scheduler.sqrt_inv_alpha[t_idx] * Z - noise_scheduler.noise_scale_coeff[t_idx] * noise\n",
    "            else:\n",
    "                Z = (noise_scheduler.sqrt_alpha_bar[s_idx] / noise_scheduler.sqrt_alpha_bar[t_idx]) * (Z - noise_scheduler.sqrt_one_minus_alpha_bar[t_idx] * noise)\n",
    "                Z = Z + noise_scheduler.sqrt_one_minus_alpha_bar[s_idx] * noise\n",
    "\n",
    "            # 途中経過の保存\n",
    "            if show_progress:\n",
    "                Y = vae_model.decode(Z)\n",
    "                Y_cpu = to_sigmoid_image(Y).to('cpu').detach()\n",
    "                show_images(Y_cpu, num=len(Y), num_per_row=8, title='timestep_{}'.format(t_idx+1), save_fig=False, save_dir=MODEL_DIR)\n",
    "\n",
    "    return vae_model.decode(Z)\n",
    "\n",
    "\n",
    "# DDPMによるデータ生成\n",
    "# こちらの方が拡散モデル本来の逆拡散過程\n",
    "def LDDPM_generate(Z, model, vae_model, noise_scheduler, n_timesteps, show_progress=False, show_interval=50):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t_idx in tqdm(reversed(range(0, n_timesteps)), total=n_timesteps):\n",
    "\n",
    "            # ノイズ推定\n",
    "            t = t_idx * torch.ones((len(Z),), device=Z.device).long()\n",
    "            noise = model(Z, t)\n",
    "\n",
    "            # ノイズ除去\n",
    "            Z = noise_scheduler.sqrt_inv_alpha[t_idx] * Z - noise_scheduler.noise_scale_coeff[t_idx] * noise\n",
    "            if t_idx != 0:\n",
    "                Z = Z + noise_scheduler.var_coeff[t_idx] * torch.randn_like(Z)\n",
    "\n",
    "            # 途中経過の保存\n",
    "            if show_progress and (t_idx + 1) % show_interval == 0:\n",
    "                Y = vae_model.decode(Z)\n",
    "                Y_cpu = to_sigmoid_image(Y).to('cpu').detach()\n",
    "                show_images(Y_cpu, num=len(Y), num_per_row=8, title='timestep_{}'.format(t_idx+1), save_fig=False, save_dir=MODEL_DIR)\n",
    "\n",
    "    return vae_model.decode(Z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "from mylib.utility import save_datasets, load_datasets_from_file\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "    train_dataset, _ = load_datasets_from_file(MODEL_DIR)\n",
    "    if train_dataset is None:\n",
    "        print('error: there is no checkpoint previously saved.')\n",
    "        exit()\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # CSVファイルを読み込み, 訓練データセットを用意\n",
    "    train_dataset = CSVBasedDataset(\n",
    "        filename = DATASET_CSV,\n",
    "        items = [\n",
    "            'File Path' # X\n",
    "        ],\n",
    "        dtypes = [\n",
    "            'image' # Xの型\n",
    "        ],\n",
    "        dirname = DATA_DIR,\n",
    "        img_transform=transforms.CenterCrop((H, W)), # 中央128ピクセル分のみを切り出して使用\n",
    "        img_range=[-1, 1],\n",
    "    )\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    save_datasets(MODEL_DIR, train_dataset)\n",
    "\n",
    "# 訓練データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VAE学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from mylib.loss_functions import VAELoss\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "# 何エポックに1回の割合で学習経過を表示するか（モデル保存処理もこれと同じ頻度で実行）\n",
    "INTERVAL_FOR_SHOWING_PROGRESS = 10\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS_FOR_VAE # 最終値\n",
    "\n",
    "# データ拡張のための画像変換処理\n",
    "image_transform = transforms.RandomHorizontalFlip(p=0.5) # 確率0.5で左右反転\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "vae_model = LDDPM_VAE(C=C, ZC=ZC).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは Adam を使用）\n",
    "optimizer = optim.AdamW(vae_model.parameters())\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, model, optimizer = load_checkpoint(VAE_CHECKPOINT_EPOCH, VAE_CHECKPOINT_MODEL, VAE_CHECKPOINT_OPT, N_EPOCHS_FOR_VAE, vae_model, optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = VAELoss(channels=C)\n",
    "\n",
    "# 損失関数値の可視化器を準備\n",
    "loss_viz = LossVisualizer(['train loss'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    vae_model.train()\n",
    "    sum_loss = 0\n",
    "    for X in tqdm(train_dataloader):\n",
    "        for param in vae_model.parameters():\n",
    "            param.grad = None\n",
    "        X = X.to(DEVICE)\n",
    "        X = image_transform(X) # データ拡張\n",
    "        Y, mu, lnvar = vae_model(X) # 入力値 X を現在のモデルに入力\n",
    "        loss = loss_func(Y, X, mu, lnvar) # 損失関数の現在値を計算\n",
    "        loss.backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "        optimizer.step() # 勾配に沿ってパラメータの値を更新\n",
    "        sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 可視化器に損失関数の値を登録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "    print('')\n",
    "\n",
    "    # 検証（学習経過の表示）\n",
    "    if epoch == 0 or (epoch + 1) % INTERVAL_FOR_SHOWING_PROGRESS == 0:\n",
    "        vae_model.eval()\n",
    "        if epoch == 0:\n",
    "            X = to_sigmoid_image(X) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "            show_images(X.to('cpu').detach(), num=len(X), num_per_row=8, title='original', save_fig=False, save_dir=MODEL_DIR) # 学習用画像の例を表示（最初のエポックのみ）\n",
    "        Y = to_sigmoid_image(Y)\n",
    "        show_images(Y.to('cpu').detach(), num=len(Y), num_per_row=8, title='epoch_{}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "\n",
    "    # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
    "    save_checkpoint(VAE_CHECKPOINT_EPOCH, VAE_CHECKPOINT_MODEL, VAE_CHECKPOINT_OPT, epoch+1, vae_model, optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "vae_model = vae_model.to('cpu')\n",
    "torch.save(vae_model.state_dict(), VAE_MODEL_FILE)\n",
    "vae_model = vae_model.to(DEVICE)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'vae_loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'vae_loss_history.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習済みVAEモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 参考までに, 教員の方で事前学習済みVAEモデルを用意しました.\n",
    "# デフォルトのニューラルネットワークモデルの下で tinyCelebA を用いて学習したものです.\n",
    "# これを用いたい場合は, 以下の変数の値を True にしてください\n",
    "LOAD_PRETRAINED_MODEL = True\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "vae_model = LDDPM_VAE(C=C, ZC=ZC).to(DEVICE)\n",
    "if LOAD_PRETRAINED_MODEL:\n",
    "    if not os.path.isfile('LDDPM_pretrained_vae_model_tinyCelebA.pth'):\n",
    "        # Windowsの場合\n",
    "        #!Powershell.exe -Command \"wget https://tus.box.com/shared/static/2crzkyk8hyaqbgikbxc0i4requw06c7t.pth -O LDDPM_pretrained_vae_model_tinyCelebA.pth\"\n",
    "        # Linux, Macの場合\n",
    "        !wget \"https://tus.box.com/shared/static/2crzkyk8hyaqbgikbxc0i4requw06c7t.pth\" -O LDDPM_pretrained_vae_model_tinyCelebA.pth\n",
    "    vae_model.load_state_dict(torch.load('LDDPM_pretrained_vae_model_tinyCelebA.pth'))\n",
    "else:\n",
    "    vae_model.load_state_dict(torch.load(VAE_MODEL_FILE)) # 最終モデルをロードする場合\n",
    "    #vae_model.load_state_dict(torch.load(autosaved_model_name(VAE_MODEL_FILE, 500))) # 例えば500エポック目のモデルをロードしたい場合は，このようにする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UNet学習処理の実行\n",
    "- 拡散モデルの学習には一般に数百〜1000エポック程度が必要となります（大抵の場合, 1日以上プログラムを回し続けることになります）. \n",
    "- 最初の10〜20エポック程度で損失関数の値は十分に下がったように見えるかもしれませんが, そこから先の僅かな上積みが生成画像の品質に大きく影響します.\n",
    "- Paperspace Gradient などのクラウド環境で一気に実行するのは困難だと思いますので, 何回かに分けて少しずつ実行することをおすすめします."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images, to_sigmoid_image, autosaved_model_name\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "# 何エポックに1回の割合で学習経過を表示するか（モデル保存処理もこれと同じ頻度で実行）\n",
    "INTERVAL_FOR_SHOWING_PROGRESS = 10\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS_FOR_UNET # 最終値\n",
    "\n",
    "# データ拡張のための画像変換処理\n",
    "image_transform = transforms.RandomHorizontalFlip(p=0.5) # 確率0.5で左右反転\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "model = LDDPM_UNet(ZC=ZC, time_embed_dim=TIME_EMBED_DIM).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは Adam を使用）\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00002)\n",
    "if not RESTART_MODE:\n",
    "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda i: min((i + 1) / 5000, 1.0)) # 学習率のウォームアップに使用\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, model, optimizer = load_checkpoint(UNET_CHECKPOINT_EPOCH, UNET_CHECKPOINT_MODEL, UNET_CHECKPOINT_OPT, N_EPOCHS_FOR_UNET, model, optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# 検証の際に使用する乱数を用意\n",
    "Z_valid = torch.randn((BATCH_SIZE, ZC, ZH, ZW)).to(DEVICE)\n",
    "\n",
    "# 損失関数値の可視化器を準備\n",
    "loss_viz = LossVisualizer(['train loss'], init_epoch=INIT_EPOCH, log_mode=True)\n",
    "\n",
    "# ノイズスケジューラを準備\n",
    "noise_scheduler = NoiseScheduler(device=DEVICE, method='linear', num_timesteps=N_TIMESTEPS)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "vae_model.eval()\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    for X in tqdm(train_dataloader):\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        X = image_transform(X) # データ拡張\n",
    "        X0 = X.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            X0 = vae_model.encode(X0, testmode=True) # 入力画像をエンコードし潜在特徴マップを取得\n",
    "        t = torch.randint(0, N_TIMESTEPS, (len(X0),), device=DEVICE).long() # タイムステップ情報をバッチごとにランダムに設定\n",
    "        noise = torch.randn_like(X0) # 正規乱数に従うノイズを用意\n",
    "        Xt = noise_scheduler.get_noisy_sample(X0, t, noise) # 用意したノイズを付加\n",
    "        noise_estimated = model(Xt, t) # U-Netを用いてノイズを推定\n",
    "        loss = loss_func(noise_estimated, noise) # 損失関数の現在値を計算\n",
    "        loss.backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "        optimizer.step() # 勾配に沿ってパラメータの値を更新\n",
    "        if not RESTART_MODE:\n",
    "            lr_scheduler.step() # 学習率のウォームアップ（エポックごとに学習率を変更）\n",
    "        sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 可視化器に損失関数の値を登録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "    print('')\n",
    "\n",
    "    # 検証（学習経過の表示，モデル自動保存）\n",
    "    if epoch == 0 or (epoch + 1) % INTERVAL_FOR_SHOWING_PROGRESS == 0:\n",
    "        model.eval()\n",
    "        if epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                X0 = vae_model.decode(X0)\n",
    "            X0 = to_sigmoid_image(X0) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "            show_images(X0.to('cpu').detach(), num=len(X0), num_per_row=8, title='real images', save_fig=False, save_dir=MODEL_DIR) # Real画像の例を表示（最初のエポックのみ）\n",
    "        with torch.inference_mode():\n",
    "            Y = LDDIM_generate(Z_valid, model, vae_model, noise_scheduler, n_timesteps=N_TIMESTEPS, n_gen_timesteps=50)\n",
    "            #Y = LDDIM_generate(torch.randn((BATCH_SIZE, ZC, ZH, ZW)).to(DEVICE), model, vae_model, noise_scheduler, n_timesteps=N_TIMESTEPS, n_gen_timesteps=50) # エポックごとに異なる乱数を使用する場合はこのようにする\n",
    "        Y_cpu = to_sigmoid_image(Y).to('cpu').detach() # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "        show_images(Y_cpu, num=len(Y), num_per_row=8, title='epoch_{}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "        torch.save(model.state_dict(), autosaved_model_name(UNET_MODEL_FILE, epoch + 1)) # 学習途中のモデルを保存したい場合はこのようにする\n",
    "\n",
    "    # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
    "    save_checkpoint(UNET_CHECKPOINT_EPOCH, UNET_CHECKPOINT_MODEL, UNET_CHECKPOINT_OPT, epoch+1, model, optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "model = model.to('cpu')\n",
    "torch.save(model.state_dict(), UNET_MODEL_FILE)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'unet_loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'unet_loss_history.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習済みUNetモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 参考までに, 教員の方で事前学習済みUNetモデルを用意しました.\n",
    "# デフォルトのニューラルネットワークモデルの下で tinyCelebA を用いて学習したものです.\n",
    "# これを用いたい場合は, 以下の変数の値を True にしてください\n",
    "LOAD_PRETRAINED_MODEL = True\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "model = LDDPM_UNet(ZC=ZC, time_embed_dim=TIME_EMBED_DIM).to(DEVICE)\n",
    "if LOAD_PRETRAINED_MODEL:\n",
    "    if not os.path.isfile('LDDPM_pretrained_unet_model_tinyCelebA.pth'):\n",
    "        # Windowsの場合\n",
    "        #!Powershell.exe -Command \"wget https://tus.box.com/shared/static/qjvf9u52kptzmwi6tp4x9u9rtua6i25p.pth -O LDDPM_pretrained_unet_model_tinyCelebA.pth\"\n",
    "        # Linux, Macの場合\n",
    "        !wget \"https://tus.box.com/shared/static/qjvf9u52kptzmwi6tp4x9u9rtua6i25p.pth\" -O LDDPM_pretrained_unet_model_tinyCelebA.pth\n",
    "    model.load_state_dict(torch.load('LDDPM_pretrained_unet_model_tinyCelebA.pth'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(UNET_MODEL_FILE)) # 最終モデルをロードする場合\n",
    "    #model.load_state_dict(torch.load(autosaved_model_name(UNET_MODEL_FILE, 500))) # 例えば500エポック目のモデルをロードしたい場合は，このようにする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理（正規分布に従ってランダムサンプリングした乱数から逆拡散過程に従って画像を生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 32\n",
    "\n",
    "# 標準正規分布 N(0, 1^2) に従って適当に乱数画像を作成\n",
    "Z = torch.randn((n_gen, ZC, ZH, ZW)).to(DEVICE)\n",
    "\n",
    "# ノイズスケジューラを準備\n",
    "noise_scheduler = NoiseScheduler(device=DEVICE, method='linear', num_timesteps=N_TIMESTEPS)\n",
    "\n",
    "# 生成処理（逆拡散過程）を実行し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    Y = LDDIM_generate(Z, model, vae_model, noise_scheduler, n_timesteps=N_TIMESTEPS, n_gen_timesteps=N_GEN_TIMESTEPS, show_progress=True) # 少ないタイムステップ数で簡易的に生成する場合\n",
    "    #Y = LDDPM_generate(Z, model, vae_model, noise_scheduler, n_timesteps=N_TIMESTEPS, show_progress=True) # 本来の逆拡散過程で生成する場合\n",
    "    Y_cpu = to_sigmoid_image(Y).to('cpu').detach()\n",
    "    show_images(Y_cpu, num=len(Y), num_per_row=8, title='LDDPM_sample_generated', save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
