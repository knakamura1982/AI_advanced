{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ed54f62-561d-4d70-b5e0-ada2dcfa6985",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b6c35a-36e5-4804-88a9-05845f930ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:04:05.739061Z",
     "iopub.status.busy": "2023-03-29T04:04:05.738572Z",
     "iopub.status.idle": "2023-03-29T04:04:05.745750Z",
     "shell.execute_reply": "2023-03-29T04:04:05.744240Z",
     "shell.execute_reply.started": "2023-03-29T04:04:05.739033Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "# 再開モードの場合も, このエポック数の分だけ追加学習される（N_EPOCHSは最終エポック番号ではない）\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "TRAIN_DATASET_CSV = './tinySTL10/train_list.csv'\n",
    "\n",
    "# テストデータセット（画像ファイルリスト）のファイル名\n",
    "TEST_DATASET_CSV = './tinySTL10/test_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './tinySTL10/'\n",
    "\n",
    "# 画像サイズ\n",
    "H = 96 # 縦幅（実際にはニューラルネットワーク内部の前処理にて 224x224 にリサイズするので, この値は使わない）\n",
    "W = 96 # 横幅（同上）\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './ViT_models/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, 'ViT_object_recognizer_model.pth')\n",
    "\n",
    "# 損失関数としてBCE損失を使用するか否か\n",
    "# これを False にすると通常のクロスエントロピー損失が使用される\n",
    "USE_BCE_LOSS = True\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイル\n",
    "CHECKPOINT_EPOCH = os.path.join(MODEL_DIR, 'checkpoint_epoch.pkl')\n",
    "CHECKPOINT_MODEL = os.path.join(MODEL_DIR, 'checkpoint_model.pth')\n",
    "CHECKPOINT_OPT = os.path.join(MODEL_DIR, 'checkpoint_opt.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "222c6861-3a41-453a-934e-8f74004239cf",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b252d113-69ea-4de5-9822-b45319be2050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:10:01.887578Z",
     "iopub.status.busy": "2023-03-29T04:10:01.887193Z",
     "iopub.status.idle": "2023-03-29T04:10:01.898772Z",
     "shell.execute_reply": "2023-03-29T04:10:01.897491Z",
     "shell.execute_reply.started": "2023-03-29T04:10:01.887550Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 画像のパッチ分割および各パッチの平坦化を実行するクラス\n",
    "#   - in_channels: 入力画像のチャンネル数（グレースケール画像なら1, カラー画像なら3）\n",
    "#   - patch_size: パッチのサイズ（入力画像の縦横幅がパッチサイズの整数倍となるように設定すること．なお，パッチは正方形とする）\n",
    "#   - layer_norm: パッチ分割後にLayer Normalizationを実行するか否か（実行する場合は True, 実行しない場合は False を指定）\n",
    "class ToFlattenedPatches(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, layer_norm=True):\n",
    "        super(ToFlattenedPatches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        if layer_norm:\n",
    "            self.ln = nn.LayerNorm([in_channels*(patch_size**2)])\n",
    "        else:\n",
    "            self.ln = None\n",
    "    def forward(self, x):\n",
    "        h = torch.cat([\n",
    "            torch.cat([\n",
    "                torch.reshape(q, (len(q), 1, -1)) for q in torch.split(p, self.patch_size, dim=3)\n",
    "            ], dim=1) for p in torch.split(x, self.patch_size, dim=2)\n",
    "        ], dim=1) # パッチ分割 + 各パッチ平坦化\n",
    "        if self.ln is not None:\n",
    "            h = self.ln(h) # Layer Normalization\n",
    "        return h\n",
    "\n",
    "\n",
    "# Linear Projectionによる各パッチの埋め込みを実行するクラス\n",
    "#   - patch_dim: 埋め込み前のパッチの次元数（パッチ分割前の画像のチャンネル数 * パッチサイズの2乗）\n",
    "#   - embed_dim: 埋め込み後のパッチ特徴量の次元数\n",
    "#   - add_cls_token: CLSトークンを付加するか否か（付加する場合は True, 付加しない場合は False を指定）\n",
    "#   - layer_norm: 埋め込み後にLayer Normalizationを実行するか否か（実行する場合は True, 実行しない場合は False を指定）\n",
    "#   - bias: Linear Projectionのパラメータにバイアスベクトルを含めるか否か（含める場合は True, 含めない場合は False を指定）\n",
    "class LinearProjection(nn.Module):\n",
    "    def __init__(self, patch_dim, embed_dim=128, add_cls_token=False, layer_norm=True, bias=True):\n",
    "        super(LinearProjection, self).__init__()\n",
    "        self.linear_projection= nn.Linear(in_features=patch_dim, out_features=embed_dim, bias=bias)\n",
    "        if add_cls_token:\n",
    "            self.cls = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.cls = None\n",
    "        if layer_norm:\n",
    "            self.ln = nn.LayerNorm([embed_dim])\n",
    "        else:\n",
    "            self.ln = None\n",
    "    def forward(self, x):\n",
    "        h = self.linear_projection(x) # 平坦化後の各パッチを埋め込みベクトルに変換\n",
    "        if self.ln is not None:\n",
    "            h = self.ln(h) # Layer Normalization\n",
    "        if self.cls is not None:\n",
    "            h = torch.cat([self.cls.repeat_interleave(x.size()[0], dim=0), h], dim=1) # CLSトークン付加\n",
    "        return h\n",
    "\n",
    "\n",
    "# 位置エンコーディングの付加処理を実行するクラス\n",
    "#   - n_patches: 想定されるパッチの数\n",
    "#   - embed_dim: 埋め込み後のパッチ特徴量の次元数（偶数を想定）\n",
    "#   - trainable: 位置エンコーディングを学習可能なパラメータとして設定するか否か（学習する場合は True, 学習しない場合は False を指定）\n",
    "#   - random_initialize: パラメータをランダムに初期化するか，三角関数に基づいて初期化するか（trainable=False の場合は常に三角関数に基づいて設定される）\n",
    "class PositionEmbeddings(nn.Module):\n",
    "    def __init__(self, n_patches=256, embed_dim=128, trainable=False, random_initialize=False):\n",
    "        super(PositionEmbeddings, self).__init__()\n",
    "        if trainable and random_initialize:\n",
    "            self.embeddings = nn.Parameter(torch.randn(1, n_patches, embed_dim))\n",
    "        else:\n",
    "            d = embed_dim // 2\n",
    "            t = torch.arange(n_patches).repeat(1, 1)\n",
    "            e = torch.log(torch.tensor(10000, device=t.device)) / d\n",
    "            e = torch.exp(torch.arange(d, device=t.device) * -e)\n",
    "            e = torch.unsqueeze(t, dim=-1) * e.repeat([1 for i in t.size()])\n",
    "            self.embeddings = nn.Parameter(torch.cat((e.sin(), e.cos()), dim=-1))\n",
    "        if not trainable:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    def forward(self):\n",
    "        return self.embeddings\n",
    "\n",
    "\n",
    "# Transformer Encoderの構成単位となる層\n",
    "# 下記の TransformerEncoder クラスの内部で使用\n",
    "class EncodingLayer(nn.Module):\n",
    "    def __init__(self, num_heads=16, embed_dim=128, ffn_dim=128, ffn_dropout_ratio=0.1):\n",
    "        super(EncodingLayer, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm([embed_dim])\n",
    "        self.ln2 = nn.LayerNorm([embed_dim])\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        if 0 < ffn_dropout_ratio and ffn_dropout_ratio <= 0.5:\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(in_features=embed_dim, out_features=ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(p=ffn_dropout_ratio),\n",
    "                nn.Linear(in_features=ffn_dim, out_features=embed_dim),\n",
    "                nn.Dropout(p=ffn_dropout_ratio),\n",
    "            )\n",
    "        else:\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(in_features=embed_dim, out_features=ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(in_features=ffn_dim, out_features=embed_dim),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        h = self.ln1(x) # Layer Normalization\n",
    "        h, _ = self.mha(h, h, h) # Multihead Attention\n",
    "        x = h + x # skip connection\n",
    "        h = self.ln2(x) # Layer Normalization\n",
    "        h = self.ffn(h) # Feed Forward Network\n",
    "        y = h + x # skip connection\n",
    "        return y\n",
    "\n",
    "\n",
    "# Transformer Encoderの処理を実行するクラス\n",
    "#   - num_layers: (Multi-Head Attention + Feed Forward Network)を何層分実行するか\n",
    "#   - num_heads: Multi-Head Attentionにおけるヘッド数\n",
    "#   - embed_dim: 埋め込み後のパッチ特徴量の次元数\n",
    "#   - ffn_dim: Feed Forward Network内部における中間層のユニット数\n",
    "#   - ffn_dropout_ratio: Feed Forward Network内部におけるドロップアウト層のドロップアウト率\n",
    "#   - layer_norm: 最後ににLayer Normalizationを実行するか否か（実行する場合は True, 実行しない場合は False を指定）\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=6, num_heads=16, embed_dim=128, ffn_dim=128, ffn_dropout_ratio=0.1, layer_norm=True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers = [EncodingLayer(num_heads, embed_dim, ffn_dim, ffn_dropout_ratio) for i in range(num_layers)]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        if layer_norm:\n",
    "            self.ln = nn.LayerNorm([embed_dim])\n",
    "        else:\n",
    "            self.ln = None\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        if self.ln is not None:\n",
    "            h = self.ln(h) # Layer Normalization\n",
    "        return h\n",
    "\n",
    "\n",
    "# Vision TransformerによりSTL10物体画像認識AIを実現するニューラルネットワーク\n",
    "class SimpleViT(nn.Module):\n",
    "\n",
    "    # C: 入力画像のチャンネル数\n",
    "    # N: 認識対象となるクラスの数\n",
    "    # image_size: 画像を最初に正方形にリサイズする際の一辺の長さ\n",
    "    # patch_size: 画像を正方形のパッチに分割する際のパッチサイズ（各パッチの一辺の長さ）\n",
    "    # num_layers: (Multi-Head Attention + Feed Forward Network)を何層分実行するか\n",
    "    # num_heads: Multi-Head Attentionにおけるヘッドの数\n",
    "    # embed_dim: 各パッチを何次元のベクトル（埋め込みベクトル）で表現するか\n",
    "    # ffn_dim: Feed Forward Networkにおける中間層のユニット数\n",
    "    # dropout_ratio: Feed Forward Networkにおけるドロップアウト層のドロップアウト率\n",
    "    def __init__(self, C, N, image_size=224, patch_size=16, num_layers=6, num_heads=8, embed_dim=128, ffn_dim=128, dropout_ratio=0.0):\n",
    "        super(SimpleViT, self).__init__()\n",
    "\n",
    "        # データ拡張\n",
    "        self.data_augment = transforms.Compose(torch.nn.ModuleList([\n",
    "            transforms.RandomHorizontalFlip(p=0.5), # 確率0.5で左右反転\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.15, saturation=0.1, hue=0.05), # カラージッター（明度を±20%，コントラストを±15%，彩度を±10%，色相を±5%の範囲内でランダムに変更）\n",
    "            transforms.RandomAffine(degrees=(-15, 15), scale=(0.8, 1.2), translate=(0.1, 0.1)), # -15度～15度の範囲でランダムに回転，±10%の範囲でランダムに平行移動，さらに80%～120%の範囲内でランダムにスケーリング\n",
    "            transforms.RandomErasing(p=0.5), # 確率0.5で一部を消去\n",
    "        ]))\n",
    "\n",
    "        # 前処理\n",
    "        self.preprocess = transforms.Resize(image_size, antialias=False) # 入力画像を正方形にリサイズ\n",
    "\n",
    "        # パッチ分割・埋め込み\n",
    "        self.patch_embed = nn.Sequential(\n",
    "            ToFlattenedPatches(in_channels=C, patch_size=patch_size, layer_norm=True),\n",
    "            LinearProjection(patch_dim=C*(patch_size**2), embed_dim=embed_dim, add_cls_token=True, layer_norm=True),\n",
    "        )\n",
    "\n",
    "        # 位置エンコーディング付加\n",
    "        self.pos_embed = PositionEmbeddings(embed_dim=embed_dim, n_patches=(image_size//patch_size)**2, trainable=False)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            embed_dim=embed_dim,\n",
    "            ffn_dim=ffn_dim,\n",
    "            ffn_dropout_ratio=dropout_ratio,\n",
    "            layer_norm=True\n",
    "        )\n",
    "\n",
    "        # クラス分類のためのMLPヘッド\n",
    "        self.mlp = nn.Linear(in_features=embed_dim, out_features=N)\n",
    "\n",
    "    def forward(self, x, testmode=False):\n",
    "        if not testmode:\n",
    "            x = self.data_augment(x) # 訓練時のみデータ拡張（テスト時は実行しない）\n",
    "\n",
    "        # 前処理（入力画像を正方形にリサイズ）\n",
    "        h = self.preprocess(x)\n",
    "\n",
    "        # 入力画像をパッチ分割し，各パッチを埋め込みベクトルに変換\n",
    "        h = self.patch_embed(h)\n",
    "\n",
    "        # 位置エンコーディングの情報を付加（ただし，先頭のCLSトークンには位置エンコーディングを付加しない）\n",
    "        h[:, 1:] += self.pos_embed()\n",
    "\n",
    "        # （マルチヘッドアテンション + MLP）× num_layers\n",
    "        h = self.transformer_encoder(h)\n",
    "\n",
    "        # 先頭のCLSトークンに対応する情報のみを取り出す\n",
    "        h = h[:, 0]\n",
    "\n",
    "        # 最終層\n",
    "        y = self.mlp(h)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdaa70a0-4b05-478c-a8f3-fee09d1edce8",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582c6bf-24d7-4f97-b34c-6a0b4118b8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:04:59.712302Z",
     "iopub.status.busy": "2023-03-29T04:04:59.711915Z",
     "iopub.status.idle": "2023-03-29T04:04:59.780017Z",
     "shell.execute_reply": "2023-03-29T04:04:59.778964Z",
     "shell.execute_reply.started": "2023-03-29T04:04:59.712273Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "from mylib.utility import save_datasets, load_datasets_from_file\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "    train_dataset, valid_dataset = load_datasets_from_file(MODEL_DIR)\n",
    "    if train_dataset is None:\n",
    "        print('error: there is no checkpoint previously saved.')\n",
    "        exit()\n",
    "    train_size = len(train_dataset)\n",
    "    valid_size = len(valid_dataset)\n",
    "    with open(os.path.join(MODEL_DIR, 'fdicts.pkl'), 'rb') as fdicts_file:\n",
    "        fdicts = pickle.load(fdicts_file)\n",
    "    n_classes = len(fdicts[1])\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # CSVファイルを読み込み, 訓練データセットを用意\n",
    "    dataset = CSVBasedDataset(\n",
    "        filename = TRAIN_DATASET_CSV,\n",
    "        items = [\n",
    "            'File Path', # X\n",
    "            'Class Name', # Y\n",
    "        ],\n",
    "        dtypes = [\n",
    "            'image', # Xの型\n",
    "            'label', # Yの型\n",
    "        ],\n",
    "        dirname = DATA_DIR,\n",
    "        img_mode = 'color', # 強制的にカラー画像として読み込む\n",
    "    )\n",
    "    with open(os.path.join(MODEL_DIR, 'fdicts.pkl'), 'wb') as fdicts_file:\n",
    "        pickle.dump(dataset.forward_dicts, fdicts_file)\n",
    "\n",
    "    # 認識対象のクラス数を取得\n",
    "    n_classes = len(dataset.forward_dicts[1])\n",
    "\n",
    "    # 訓練データセットを分割し，一方を検証用に回す\n",
    "    dataset_size = len(dataset)\n",
    "    valid_size = int(0.05 * dataset_size) # 全体の 5% を検証用に\n",
    "    train_size = dataset_size - valid_size # 残りの 95% を学習用に\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    save_datasets(MODEL_DIR, train_dataset, valid_dataset)\n",
    "\n",
    "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b1aea1-4ab4-41ed-9dcb-9874f4c193af",
   "metadata": {},
   "source": [
    "##### 学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ca80c-4186-4d86-9663-e1a266cc522b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:10:10.524021Z",
     "iopub.status.busy": "2023-03-29T04:10:10.523112Z",
     "iopub.status.idle": "2023-03-29T04:11:42.906423Z",
     "shell.execute_reply": "2023-03-29T04:11:42.904894Z",
     "shell.execute_reply.started": "2023-03-29T04:10:10.524021Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "model = SimpleViT(C=C, N=n_classes, image_size=112, patch_size=14, num_layers=6, num_heads=16, embed_dim=128, ffn_dim=128, dropout_ratio=0.1).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, model, optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_MODEL, CHECKPOINT_OPT, N_EPOCHS, model, optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = nn.BCEWithLogitsLoss() if USE_BCE_LOSS else nn.CrossEntropyLoss()\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['train loss', 'valid loss', 'valid accuracy'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    for X, Y in tqdm(train_dataloader): # X, Y は CSVBasedDataset クラスの __getitem__ 関数の戻り値に対応\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        Y_pred = model(X) # 入力画像 X を現在のニューラルネットワークに入力し，出力の推定値を得る\n",
    "        if USE_BCE_LOSS:\n",
    "            loss = loss_func(Y_pred, F.one_hot(Y, num_classes=n_classes).to(torch.float32)) # 損失関数の現在値を計算（BCE損失を用いる場合）\n",
    "        else:\n",
    "            loss = loss_func(Y_pred, Y) # 損失関数の現在値を計算（クロスエントロピー損失を用いる場合）\n",
    "        loss.backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "        optimizer.step() # 勾配に沿ってパラメータの値を更新\n",
    "        sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 訓練データに対する損失関数の値を記録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "\n",
    "    # 検証\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    n_failed = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, Y in tqdm(valid_dataloader):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            Y_pred = model(X, testmode=True)\n",
    "            if USE_BCE_LOSS:\n",
    "                loss = loss_func(Y_pred, F.one_hot(Y, num_classes=n_classes).to(torch.float32))\n",
    "            else:\n",
    "                loss = loss_func(Y_pred, Y)\n",
    "            sum_loss += float(loss) * len(X)\n",
    "            n_failed += float(torch.count_nonzero(torch.argmax(Y_pred, dim=1) - Y)) # 推定値と正解値が一致していないデータの個数を数える\n",
    "    avg_loss = sum_loss / valid_size\n",
    "    accuracy = (valid_size - n_failed) / valid_size\n",
    "    loss_viz.add_value('valid loss', avg_loss) # 検証用データに対する損失関数の値を記録\n",
    "    loss_viz.add_value('valid accuracy', accuracy) # 検証用データに対する認識精度の値を記録\n",
    "    print('valid loss = {0:.6f}'.format(avg_loss))\n",
    "    print('accuracy = {0:.2f}%'.format(100 * accuracy))\n",
    "    print('')\n",
    "\n",
    "    # 現在の学習状態を一時ファイルに保存\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_MODEL, CHECKPOINT_OPT, epoch+1, model, optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "model = model.to('cpu')\n",
    "torch.save(model.state_dict(), MODEL_FILE)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf1e1f4",
   "metadata": {},
   "source": [
    "##### テストデータセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9486051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "\n",
    "\n",
    "# CSVファイルを読み込み, テストデータセットを用意\n",
    "with open(os.path.join(MODEL_DIR, 'fdicts.pkl'), 'rb') as fdicts_file:\n",
    "    fdicts = pickle.load(fdicts_file)\n",
    "test_dataset = CSVBasedDataset(\n",
    "    filename = TEST_DATASET_CSV,\n",
    "    items = [\n",
    "        'File Path', # X\n",
    "        'Class Name', # Y\n",
    "    ],\n",
    "    dtypes = [\n",
    "        'image', # Xの型\n",
    "        'label', # Yの型\n",
    "    ],\n",
    "    dirname = DATA_DIR,\n",
    "    fdicts = fdicts,\n",
    "    img_mode = 'color', # 強制的にカラー画像として読み込む\n",
    ")\n",
    "test_size = len(test_dataset)\n",
    "rdict = test_dataset.reverse_dicts[1]\n",
    "\n",
    "# 認識対象のクラス数を取得\n",
    "n_classes = len(test_dataset.forward_dicts[1])\n",
    "\n",
    "# テストデータをミニバッチに分けて使用するための「データローダ」を用意\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1391d377",
   "metadata": {},
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb382861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "model = SimpleViT(C=C, N=n_classes, image_size=112, patch_size=14, num_layers=6, num_heads=16, embed_dim=128, ffn_dim=128, dropout_ratio=0.1)\n",
    "model.load_state_dict(torch.load(MODEL_FILE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "882dbc96",
   "metadata": {},
   "source": [
    "##### 単一画像に対するテスト処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_single_image\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# index 番目のテストデータをニューラルネットワークに入力してみる\n",
    "while True:\n",
    "    print('index?: ', end='')\n",
    "    val = input()\n",
    "    if val == 'exit': # 'exit' とタイプされたら終了\n",
    "        break\n",
    "    index = int(val)\n",
    "    x, y = test_dataset[index]\n",
    "    x = x.reshape(1, *x.size()).to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model(x, testmode=True)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print('')\n",
    "    print('estimated:', rdict[int(y_pred)])\n",
    "    print('ground truth:', rdict[int(y)])\n",
    "    print('')\n",
    "    show_single_image(x.to('cpu'), title='input image', sec=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "489b7496",
   "metadata": {},
   "source": [
    "##### 全ての画像に対するテスト処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# テストデータセットを用いて認識精度を評価\n",
    "n_failed = 0\n",
    "with torch.inference_mode():\n",
    "    for X, Y in tqdm(test_dataloader):\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        Y_pred = model(X, testmode=True)\n",
    "        n_failed += torch.count_nonzero(torch.argmax(Y_pred, dim=1) - Y) # 推定値と正解値が一致していないデータの個数を数える\n",
    "    accuracy = (test_size - n_failed) / test_size\n",
    "    print('accuracy = {0:.2f}%'.format(100 * accuracy))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
