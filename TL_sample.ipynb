{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ed54f62-561d-4d70-b5e0-ada2dcfa6985",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6c35a-36e5-4804-88a9-05845f930ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:04:05.739061Z",
     "iopub.status.busy": "2023-03-29T04:04:05.738572Z",
     "iopub.status.idle": "2023-03-29T04:04:05.745750Z",
     "shell.execute_reply": "2023-03-29T04:04:05.744240Z",
     "shell.execute_reply.started": "2023-03-29T04:04:05.739033Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "TRAIN_DATASET_CSV = './tinySTL10/train_list.csv'\n",
    "\n",
    "# テストデータセット（画像ファイルリスト）のファイル名\n",
    "TEST_DATASET_CSV = './tinySTL10/test_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './tinySTL10'\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './TL_models'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, 'TL_object_recognizer_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66a21794",
   "metadata": {},
   "source": [
    "##### バックボーンモデルとして使用する ResNet18 のロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a95d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# ResNet18のモデルをロード\n",
    "# 参考： https://pytorch.org/vision/stable/models.html\n",
    "resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4058463a",
   "metadata": {},
   "source": [
    "##### ResNet18のモデル構造の出力（転移学習に際し，どの層を出力を流用するかを決めるため，構造を確認する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b356a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c28434a2",
   "metadata": {},
   "source": [
    "##### 流用する層の決定，および，その層の出力サイズの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "\n",
    "# 流用する層の名前（例えば 'layer4' を流用する場合）\n",
    "layer_name = 'layer4'\n",
    "\n",
    "# ResNet18の 'layer4' 層の出力を 'feature' という名前で取得できるようにし，\n",
    "# それを backbone という変数に保存しておく\n",
    "backbone = create_feature_extractor(resnet, {layer_name: 'feature'})\n",
    "\n",
    "# 224x224 ピクセルのランダムなカラー画像を用意し，それを backbone に入力してみる．\n",
    "# これにより，流用する層の出力サイズ（パーセプトロンがどのような並びになっているか）を調べる．\n",
    "# [1, 512, 7, 7] のように出力されたら，サイズ 512x7x7 の特徴マップ（チャンネル数 512, 縦幅 7, 横幅 7 の特徴マップ）が出力される，ということ\n",
    "x = torch.rand((1, 3, 224, 224), dtype=torch.float32)\n",
    "h = backbone(x)['feature']\n",
    "print(h.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "222c6861-3a41-453a-934e-8f74004239cf",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252d113-69ea-4de5-9822-b45319be2050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:10:01.887578Z",
     "iopub.status.busy": "2023-03-29T04:10:01.887193Z",
     "iopub.status.idle": "2023-03-29T04:10:01.898772Z",
     "shell.execute_reply": "2023-03-29T04:10:01.897491Z",
     "shell.execute_reply.started": "2023-03-29T04:10:01.887550Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 畳込み，バッチ正規化，ReLUをセットで行うクラス\n",
    "class myConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(myConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "# STL10物体画像認識AIを実現するニューラルネットワーク（転移学習バージョン）\n",
    "class STL10Recognizer(nn.Module):\n",
    "\n",
    "    # N: 認識対象となるクラスの数\n",
    "    def __init__(self, N):\n",
    "        super(STL10Recognizer, self).__init__()\n",
    "\n",
    "        # 前処理\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, antialias=False), # 画像サイズを 224x224 ピクセルにリサイズする\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 取り出した部分の画素値を正規化する（ResNetと同じ条件にするため）\n",
    "        ])\n",
    "\n",
    "        # バックボーンモデルの登録\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # バックボーンモデルのパラメータを固定（これをしない場合はファインチューニングになる）\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 畳込み層\n",
    "        # 例えば 'layer4' 層を流用する場合，その出力サイズが 512x7x7 なので，in_channels を 512 にする．\n",
    "        # なお，その他の設定値が kernel_size=3, stride=1, padding=0 となっているので，この層を通した後のサイズは 64x5x5 になる\n",
    "        self.conv = myConv2d(in_channels=512, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        # 平坦化\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # 全結合層\n",
    "        self.fc1 = nn.Linear(in_features=64*5*5, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.preprocess(x)\n",
    "        h = self.backbone(h)['feature']\n",
    "        h = self.conv(h)\n",
    "        h = self.flat(h)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        y = self.fc2(h)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdaa70a0-4b05-478c-a8f3-fee09d1edce8",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582c6bf-24d7-4f97-b34c-6a0b4118b8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:04:59.712302Z",
     "iopub.status.busy": "2023-03-29T04:04:59.711915Z",
     "iopub.status.idle": "2023-03-29T04:04:59.780017Z",
     "shell.execute_reply": "2023-03-29T04:04:59.778964Z",
     "shell.execute_reply.started": "2023-03-29T04:04:59.712273Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "\n",
    "\n",
    "# CSVファイルを読み込み, 訓練データセットを用意\n",
    "dataset = CSVBasedDataset(\n",
    "    filename = TRAIN_DATASET_CSV,\n",
    "    items = [\n",
    "        'File Path', # X\n",
    "        'Class Name' # Y\n",
    "    ],\n",
    "    dtypes = [\n",
    "        'image', # Xの型\n",
    "        'label' # Yの型\n",
    "    ],\n",
    "    dirname = DATA_DIR,\n",
    "    img_mode = 'color' # 強制的にカラー画像として読み込む\n",
    ")\n",
    "with open(os.path.join(MODEL_DIR, 'fdicts.pkl'), 'wb') as fdicts_file:\n",
    "    pickle.dump(dataset.forward_dicts, fdicts_file)\n",
    "\n",
    "# 認識対象のクラス数を取得\n",
    "n_classes = len(dataset.forward_dicts[1])\n",
    "\n",
    "# 訓練データセットを分割し，一方を検証用に回す\n",
    "dataset_size = len(dataset)\n",
    "valid_size = int(0.1 * dataset_size) # 全体の 10% を検証用に\n",
    "train_size = dataset_size - valid_size # 残りの 90% を学習用に\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b1aea1-4ab4-41ed-9dcb-9874f4c193af",
   "metadata": {},
   "source": [
    "##### 学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ca80c-4186-4d86-9663-e1a266cc522b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T04:10:10.524021Z",
     "iopub.status.busy": "2023-03-29T04:10:10.523112Z",
     "iopub.status.idle": "2023-03-29T04:11:42.906423Z",
     "shell.execute_reply": "2023-03-29T04:11:42.904894Z",
     "shell.execute_reply.started": "2023-03-29T04:10:10.524021Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.visualizers import LossVisualizer\n",
    "\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "model = STL10Recognizer(N=n_classes).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 損失関数：クロスエントロピー損失を使用\n",
    "loss_func =  nn.CrossEntropyLoss()\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['train loss', 'valid loss'])\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    model.train()\n",
    "    model.backbone.eval() # ファインチューニングの場合は不要\n",
    "    sum_loss = 0\n",
    "    for X, Y in tqdm(train_dataloader): # X, Y は CSVBasedDataset クラスの __getitem__ 関数の戻り値に対応\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        Y_pred = model(X) # 入力画像 X を現在のニューラルネットワークに入力し，出力の推定値を得る\n",
    "        loss = loss_func(Y_pred, Y) # 損失関数の現在値を計算\n",
    "        loss.backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "        optimizer.step() # 勾配に沿ってパラメータの値を更新\n",
    "        sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 訓練データに対する損失関数の値を記録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "\n",
    "    # 検証\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    n_failed = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, Y in tqdm(valid_dataloader):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            Y_pred = model(X)\n",
    "            loss = loss_func(Y_pred, Y)\n",
    "            sum_loss += float(loss) * len(X)\n",
    "            n_failed += torch.count_nonzero(torch.argmax(Y_pred, dim=1) - Y) # 推定値と正解値が一致していないデータの個数を数える\n",
    "    avg_loss = sum_loss / valid_size\n",
    "    loss_viz.add_value('valid loss', avg_loss) # 検証用データに対する損失関数の値を記録\n",
    "    accuracy = (valid_size - n_failed) / valid_size\n",
    "    print('valid loss = {0:.6f}'.format(avg_loss))\n",
    "    print('accuracy = {0:.2f}%'.format(100 * accuracy))\n",
    "    print('')\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "model = model.to('cpu')\n",
    "torch.save(model.state_dict(), MODEL_FILE)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf1e1f4",
   "metadata": {},
   "source": [
    "##### テストデータセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "\n",
    "\n",
    "# CSVファイルを読み込み, テストデータセットを用意\n",
    "with open(os.path.join(MODEL_DIR, 'fdicts.pkl'), 'rb') as fdicts_file:\n",
    "    fdicts = pickle.load(fdicts_file)\n",
    "test_dataset = CSVBasedDataset(\n",
    "    filename = TEST_DATASET_CSV,\n",
    "    items = [\n",
    "        'File Path', # X\n",
    "        'Class Name' # Y\n",
    "    ],\n",
    "    dtypes = [\n",
    "        'image', # Xの型\n",
    "        'label' # Yの型\n",
    "    ],\n",
    "    dirname = DATA_DIR,\n",
    "    img_mode = 'color', # 強制的にカラー画像として読み込む\n",
    "    fdicts = fdicts\n",
    ")\n",
    "test_size = len(test_dataset)\n",
    "rdict = test_dataset.reverse_dicts[1]\n",
    "\n",
    "# 認識対象のクラス数を取得\n",
    "n_classes = len(test_dataset.forward_dicts[1])\n",
    "\n",
    "# テストデータをミニバッチに分けて使用するための「データローダ」を用意\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1391d377",
   "metadata": {},
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb382861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "model = STL10Recognizer(N=n_classes)\n",
    "model.load_state_dict(torch.load(MODEL_FILE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "882dbc96",
   "metadata": {},
   "source": [
    "##### 単一画像に対するテスト処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_single_image\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# index 番目のテストデータをニューラルネットワークに入力してみる\n",
    "while True:\n",
    "    print('index?: ', end='')\n",
    "    val = input()\n",
    "    if val == 'exit': # 'exit' とタイプされたら終了\n",
    "        break\n",
    "    index = int(val)\n",
    "    x, y = test_dataset[index]\n",
    "    x = x.reshape(1, *x.size()).to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model(x)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print('')\n",
    "    print('estimated:', rdict[int(y_pred)])\n",
    "    print('ground truth:', rdict[int(y)])\n",
    "    print('')\n",
    "    show_single_image(x.to('cpu'), title='input image', sec=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "489b7496",
   "metadata": {},
   "source": [
    "##### 全ての画像に対するテスト処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# テストデータセットを用いて認識精度を評価\n",
    "n_failed = 0\n",
    "with torch.inference_mode():\n",
    "    for X, Y in tqdm(test_dataloader):\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        Y_pred = model(X)\n",
    "        n_failed += torch.count_nonzero(torch.argmax(Y_pred, dim=1) - Y) # 推定値と正解値が一致していないデータの個数を数える\n",
    "    accuracy = (test_size - n_failed) / test_size\n",
    "    print('accuracy = {0:.2f}%'.format(100 * accuracy))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
