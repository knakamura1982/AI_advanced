{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "N_EPOCHS = 20\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "DATASET_CSV = './tinyCelebA/train_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './tinyCelebA'\n",
    "\n",
    "# 取り扱う属性ラベル\n",
    "TARGET_ATTRIBUTES = ['Blond_Hair', 'Brown_Hair', 'Black_Hair', 'Gray_Hair', 'Eyeglasses', 'Male', 'Young']\n",
    "\n",
    "# 画像サイズ\n",
    "H = 128 # 縦幅\n",
    "W = 128 # 横幅\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 特徴ベクトルの次元数\n",
    "N = 256\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './CVAE_models'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE_ENC = os.path.join(MODEL_DIR, 'cvae_encoder_model.pth') # エンコーダ\n",
    "MODEL_FILE_DEC = os.path.join(MODEL_DIR, 'cvae_decoder_model.pth') # デコーダ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Residual Block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, stride, padding, activation=F.relu):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=channels)\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.bn1(self.conv1(x)))\n",
    "        h = self.bn2(self.conv2(h))\n",
    "        return self.activation(h + x)\n",
    "\n",
    "\n",
    "# 顔画像を N 次元の特徴ベクトルへと圧縮するニューラルネットワーク（CVAE版）\n",
    "# CVAEのエンコーダ部分のサンプル\n",
    "class FaceEncoder(nn.Module):\n",
    "\n",
    "    # C: 入力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 入力顔画像の縦幅（16の倍数と仮定）\n",
    "    # W: 入力顔画像の横幅（16の倍数と仮定）\n",
    "    # N: 出力の特徴ベクトルの次元数\n",
    "    # K: 属性ラベルの種類数\n",
    "    def __init__(self, C, H, W, N, K):\n",
    "        super(FaceEncoder, self).__init__()\n",
    "\n",
    "        # 畳込み層1\n",
    "        # カーネルサイズ3，ストライド幅1，パディング1の設定なので，これを通しても特徴マップの縦幅・横幅は変化しない\n",
    "        self.conv1 = nn.Conv2d(in_channels=C, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # 畳込み層2～5\n",
    "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 1/2 になる\n",
    "        # 4つ適用することになるので，最終的には都合 1/16 になる -> ゆえに，入力顔画像の縦幅と横幅を各々16の倍数と仮定している\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # バッチ正規化層\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=128)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "        # 平坦化\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # 画像情報を処理する全結合層\n",
    "        # 畳込み層1～5を通すことにより特徴マップの縦幅・横幅は都合 1/16 になっている．\n",
    "        # したがって，入力側のパーセプトロン数は 128*(H/16)*(W/16) = H*W/2\n",
    "        self.fc_img = nn.Linear(in_features=H*W//2, out_features=256) # 画像情報は最終的に256次元に\n",
    "\n",
    "        # 属性ラベル情報を処理する全結合層\n",
    "        self.fc_lab1 = nn.Linear(in_features=K, out_features=256)\n",
    "        self.fc_lab2 = nn.Linear(in_features=256, out_features=256) # ラベル情報も256次元に\n",
    "\n",
    "        # 画像・属性ラベル情報の結合後に用いる全結合層\n",
    "        self.fc_mu = nn.Linear(in_features=512, out_features=N) # 256次元になった画像情報と属性ラベル情報を結合するので，トータルで512次元\n",
    "        self.fc_lnvar = nn.Linear(in_features=512, out_features=N)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # 画像情報 x を処理\n",
    "        h = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        h = F.leaky_relu(self.bn2(self.conv2(h)))\n",
    "        h = F.leaky_relu(self.bn3(self.conv3(h)))\n",
    "        h = F.leaky_relu(self.bn4(self.conv4(h)))\n",
    "        h = F.leaky_relu(self.bn5(self.conv5(h)))\n",
    "        h = self.flat(h)\n",
    "        hx = torch.tanh(self.fc_img(h))\n",
    "\n",
    "        # 属性ラベル情報 y を処理\n",
    "        h = F.leaky_relu(self.fc_lab1(y))\n",
    "        hy = torch.tanh(self.fc_lab2(h))\n",
    "\n",
    "        # 画像情報と属性ラベル情報を結合\n",
    "        h = torch.cat((hx, hy), dim=1)\n",
    "\n",
    "        # 特徴分布の平均・分散を計算し，特徴ベクトルを一つサンプリング\n",
    "        mu = self.fc_mu(h)\n",
    "        lnvar = self.fc_lnvar(h)\n",
    "        eps = torch.randn_like(mu) # mu と同じサイズの標準正規乱数を生成\n",
    "        z = mu + eps * torch.exp(0.5 * lnvar)\n",
    "        return z, mu, lnvar\n",
    "\n",
    "\n",
    "# N 次元の特徴ベクトルから顔画像を生成するニューラルネットワーク（CVAE版）\n",
    "# CVAEのデコーダ部分のサンプル\n",
    "class FaceDecoder(nn.Module):\n",
    "\n",
    "    # C: 出力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 出力顔画像の縦幅（16の倍数と仮定）\n",
    "    # W: 出力顔画像の横幅（16の倍数と仮定）\n",
    "    # N: 入力の特徴ベクトルの次元数\n",
    "    # K: 属性ラベルの種類数\n",
    "    def __init__(self, C, H, W, N, K):\n",
    "        super(FaceDecoder, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "\n",
    "        # 属性ラベル情報を処理する全結合層\n",
    "        self.fc_lab1 = nn.Linear(in_features=K, out_features=256)\n",
    "        self.fc_lab2 = nn.Linear(in_features=256, out_features=256) # 属性ラベル情報は最終的に256次元に\n",
    "\n",
    "        # 特徴ベクトルを処理する全結合層\n",
    "        self.fc_feat = nn.Linear(in_features=N, out_features=256) # 特徴ベクトルも256次元に\n",
    "\n",
    "        # 属性ラベル情報と特徴ベクトルの統合後に用いる全結合層\n",
    "        self.fc_all = nn.Linear(in_features=512, out_features=H*W//2) # 256次元になったラベル情報と特徴ベクトルを結合するので，トータルで512次元\n",
    "\n",
    "        # 逆畳込み層1～4\n",
    "        # カーネルサイズ，ストライド幅，パディングは FaceEncoder の畳込み層2～5と真逆に設定\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Residual Block 1〜4\n",
    "        # checker board artifact の補正を狙いとして逆畳み込み層の直後に入れる\n",
    "        self.rb1 = ResBlock(channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.rb2 = ResBlock(channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.rb3 = ResBlock(channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.rb4 = ResBlock(channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # バッチ正規化層\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=128)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=16)\n",
    "\n",
    "        # 畳込み層（最終層）\n",
    "        self.conv = nn.Conv2d(in_channels=16, out_channels=C, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "\n",
    "        # 特徴ベクトル z を処理\n",
    "        hz = torch.tanh(self.fc_feat(z))\n",
    "\n",
    "        # 属性ラベル情報 y を処理\n",
    "        h = F.leaky_relu(self.fc_lab1(y))\n",
    "        hy = torch.tanh(self.fc_lab2(h))\n",
    "\n",
    "        # 特徴ベクトルと属性ラベル情報を結合\n",
    "        h = torch.cat((hz, hy), dim=1)\n",
    "\n",
    "        # 画像を生成\n",
    "        h = F.leaky_relu(self.fc_all(h))\n",
    "        h = torch.reshape(h, (len(h), 128, self.H//16, self.W//16)) # 一列に並んだパーセプトロンを 128*(H/16)*(W/16) の特徴マップに並べ直す\n",
    "        h = F.leaky_relu(self.bn1(self.deconv1(h)))\n",
    "        h = self.rb1(h) # Residual Blockの内部でバッチ正規化と活性化関数を適用しているので，外側では適用しない\n",
    "        h = F.leaky_relu(self.bn2(self.deconv2(h)))\n",
    "        h = self.rb2(h) # 同上\n",
    "        h = F.leaky_relu(self.bn3(self.deconv3(h)))\n",
    "        h = self.rb3(h) # 同上\n",
    "        h = F.leaky_relu(self.bn4(self.deconv4(h)))\n",
    "        h = self.rb4(h) # 同上\n",
    "        y = torch.sigmoid(self.conv(h))\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "\n",
    "\n",
    "# CSVファイルを読み込み, 訓練データセットを用意\n",
    "dataset = CSVBasedDataset(\n",
    "    filename = DATASET_CSV,\n",
    "    items = [\n",
    "        'File Path', # X\n",
    "        TARGET_ATTRIBUTES # Y\n",
    "    ],\n",
    "    dtypes = [\n",
    "        'image', # Xの型\n",
    "        'float'  # Yの型\n",
    "    ],\n",
    "    dirname = DATA_DIR,\n",
    "    img_transform = transforms.CenterCrop((H, W)) # 処理量を少しでも抑えるため，画像中央の H×W ピクセルの部分だけを対象とする\n",
    ")\n",
    "\n",
    "# 訓練データセットを分割し，一方を検証用に回す\n",
    "dataset_size = len(dataset)\n",
    "valid_size = int(0.002 * dataset_size) # 全体の 0.2% を検証用に -> tinyCelebA の画像は全部で 16000 枚なので，検証用画像は 16000*0.002=32 枚\n",
    "train_size = dataset_size - valid_size # 残りの 99.8% を学習用に\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.loss_functions import VAELoss\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images\n",
    "\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "enc_model = FaceEncoder(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES)).to(DEVICE)\n",
    "dec_model = FaceDecoder(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES)).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "enc_optimizer = optim.Adam(enc_model.parameters())\n",
    "dec_optimizer = optim.Adam(dec_model.parameters())\n",
    "\n",
    "# 損失関数\n",
    "loss_func = VAELoss(channels=3, alpha=0.01)\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['train loss', 'valid loss'])\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    enc_model.train()\n",
    "    dec_model.train()\n",
    "    sum_loss = 0\n",
    "    for X, Y in tqdm(train_dataloader):\n",
    "        for param in enc_model.parameters():\n",
    "            param.grad = None\n",
    "        for param in dec_model.parameters():\n",
    "            param.grad = None\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        Z, mu, lnvar = enc_model(X, Y) # 入力画像 X と属性ラベル情報 Y を現在のエンコーダに入力し，特徴ベクトル Z を得る\n",
    "        X_rec = dec_model(Z, Y) # 特徴ベクトル Z と属性ラベル情報 Y を現在のデコーダに入力し，復元画像 X_rec を得る\n",
    "        loss = loss_func(X, X_rec, mu, lnvar) # 損失関数の現在値を計算\n",
    "        loss.backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "        enc_optimizer.step() # 勾配に沿ってパラメータの値を更新\n",
    "        dec_optimizer.step() # 同上\n",
    "        sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 訓練データに対する損失関数の値を記録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "\n",
    "    # 検証\n",
    "    enc_model.eval()\n",
    "    dec_model.eval()\n",
    "    sum_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, Y in tqdm(valid_dataloader):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            Z, mu, lnvar = enc_model(X, Y)\n",
    "            X_rec = dec_model(mu, Y) \n",
    "            loss = loss_func(X, X_rec, mu, lnvar)\n",
    "            sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / valid_size\n",
    "    loss_viz.add_value('valid loss', avg_loss) # 検証用データに対する損失関数の値を記録\n",
    "    print('valid loss = {0:.6f}'.format(avg_loss))\n",
    "    print('')\n",
    "\n",
    "    # 学習経過の表示\n",
    "    if epoch == 0:\n",
    "        show_images(X.to('cpu').detach(), num=BATCH_SIZE, num_per_row=8, title='original', save_fig=False, save_dir=MODEL_DIR)\n",
    "    show_images(X_rec.to('cpu').detach(), num=BATCH_SIZE, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "enc_model = enc_model.to('cpu')\n",
    "dec_model = dec_model.to('cpu')\n",
    "torch.save(enc_model.state_dict(), MODEL_FILE_ENC)\n",
    "torch.save(dec_model.state_dict(), MODEL_FILE_DEC)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "dec_model = FaceDecoder(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES))\n",
    "dec_model.load_state_dict(torch.load(MODEL_FILE_DEC))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理（正規分布に従って複数の乱数ベクトルをランダムサンプリングし，それをデコーダに通して画像を生成．属性ラベルは固定値で指定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_images\n",
    "\n",
    "\n",
    "dec_model = dec_model.to(DEVICE)\n",
    "dec_model.eval()\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 32\n",
    "\n",
    "# 属性ラベルの指定値\n",
    "# このサンプルコードでは TARGET_ATTRIBUTES = ['Blond_Hair', 'Brown_Hair', 'Black_Hair', 'Gray_Hair', 'Eyeglasses', 'Male', 'Young'] と設定しているので，\n",
    "#   'Blond_Hair' = 0, # ブロンド髪ではない\n",
    "#   'Brown_Hair' = 0, # 茶髪ではない\n",
    "#   'Black_Hair' = 1, # 黒髪である\n",
    "#   'Gray_Hair'  = 0, # 白髪ではない\n",
    "#   'Eyeglasses' = 0, # 眼鏡やサングラスをかけていない\n",
    "#   'Male'       = 0, # 男性でいない（== 女性）\n",
    "#   'Young'      = 1, # 若い\n",
    "# という意味になる\n",
    "attributes = [1, 0, 0, 0, 1, 0, 1]\n",
    "\n",
    "# 標準正規分布 N(0, 1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((n_gen, N)).to(DEVICE)\n",
    "\n",
    "# 属性ラベル情報の作成\n",
    "Y = torch.tensor([attributes], dtype=torch.float32).repeat((n_gen, 1)).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルと属性ラベルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    X = dec_model(Z, Y)\n",
    "    show_images(X.to('cpu').detach(), num=n_gen, num_per_row=8, title='CVAE_sample_generated_case1', save_fig=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理（乱数ベクトルを一つだけサンプリングし，それをデコーダに通して画像を生成．属性ラベルは，一つの次元を徐々に変化させる形で指定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from mylib.data_io import show_images\n",
    "\n",
    "\n",
    "dec_model = dec_model.to(DEVICE)\n",
    "dec_model.eval()\n",
    "\n",
    "# ベースとなる属性ラベル\n",
    "base_attributes = [0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "# 上の属性ラベルのうち何番目の属性値を変化させるか\n",
    "# 以下の例は\n",
    "#   - 0番目の属性（ 'Blond_Hair', ベース値 0 ）を 0 から 1 に徐々に変化\n",
    "#   - 2番目の属性（ 'Black_Hair', ベース値 1 ）を 1 から 0 に徐々に変化\n",
    "# という意味になり，すなわち，ブロンド髪から黒髪への属性変化に相当\n",
    "targets = [5]\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 16 # 上で指定した属性ラベルを 0～1 の間で n_gen 段階に変化させる\n",
    "\n",
    "# 標準正規分布 N(0, 1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((1, N)).repeat((n_gen, 1)).to(DEVICE)\n",
    "\n",
    "# 属性ラベル情報の作成\n",
    "Y = []\n",
    "for i in range(n_gen):\n",
    "    attributes = copy.deepcopy(base_attributes)\n",
    "    for t in targets:\n",
    "        # t番目の属性値を 0〜1 の範囲でずらす\n",
    "        if base_attributes[t] == 0:\n",
    "            attributes[t] = i / (n_gen - 1)\n",
    "        else:\n",
    "            attributes[t] = 1 - i / (n_gen - 1)\n",
    "    Y.append(attributes)\n",
    "Y = torch.tensor(Y, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルと属性ラベルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    X = dec_model(Z, Y)\n",
    "    show_images(X.to('cpu').detach(), num=n_gen, num_per_row=8, title='CVAE_sample_generated_case2', save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
